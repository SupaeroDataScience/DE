{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5080c7e1",
   "metadata": {},
   "source": [
    "# Welcome to the DE SDD Evaluation!\n",
    "\n",
    "Today, the goal is to understand how a distributed system can be useful when dealing with medium to large scale data sets.  \n",
    "We'll see that Dask start to be nice as soon as the Data we need to process doesn't quite fit in memory, but also if we\n",
    "need to launch several computations in parallel.\n",
    "\n",
    "In this evaluation, you will:\n",
    "- Use Dask to read and understand the several gigabytes input dataset in a interactive way,\n",
    "- Preprocess the data in a distributed way: cleaning it up and adding some useful features,\n",
    "- Launch some model training that can be parallelized on a big dataset,\n",
    "- Reduce the dataset and train more accurate models on less Data,\n",
    "- Do an hyper parameter search to find the best model on a small sample of Data.\n",
    "\n",
    "In order to run and fill this notebook, you'll need to first deploy a Dask enabled Kubernetes cluster as seen before. So please use the Kubernetes_DaskHub notebook for the steps to do it.\n",
    "\n",
    "Once the Jupyterhub is up, you can clone the DE repository from a Jupyterlab terminal to get this notebook, and select the default kernel.\n",
    "```\n",
    "git clone https://github.com/SupaeroDataScience/DE.git\n",
    "cd DE/notebooks\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f615b",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "It is some statistics about NY Taxi cabs. \n",
    "\n",
    "See https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview, or https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data.\n",
    "        \n",
    "The goal of this evaluation will be to generate a model using machine learning algorithms that will predict the fare amount\n",
    "of a taxi ride given the other input parameters we have.\n",
    "\n",
    "The model will be evaluated using the Root mean squared error algorithm:  \n",
    "https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview/evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255611eb",
   "metadata": {},
   "source": [
    "## Try to analyze the Data using Kaggles' start-up code\n",
    "\n",
    "As an introduction, we'll use Kaggle starters' code to get some insights on the data set and\n",
    "computations we'll do and measure pandas library (non parallelized access and process) performance.\n",
    "\n",
    "See https://www.kaggle.com/dster/nyc-taxi-fare-starter-kernel-simple-linear-model where this comes from.\n",
    "\n",
    "This Kaggle method will set the bar to beat with our own tools. I'm sure you can do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61175e73",
   "metadata": {},
   "source": [
    "#### Reading the data with pandas\n",
    "\n",
    "We're reading only about 20% from the whole data set. Using the storage_options kwarg was mandatory for me to avoid auth issues, don't forget it when you read public data from cloud storage during this evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a9896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "train_df =  pd.read_csv('gs://obd-dask23/train.csv', nrows = 10_000_000, storage_options={'token': 'anon'})\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df98a8b",
   "metadata": {},
   "source": [
    "#### Analysing dataset, adding some features and droping null values\n",
    "\n",
    "Let's see if we can see some correlation between passengers count and fare amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f584bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df.groupby(train_df.passenger_count).fare_amount.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6623ecc",
   "metadata": {},
   "source": [
    "Maybe adding some features about the distance of the trip could be a good idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b70ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 'abs_diff_longitude' 'abs_diff_latitude' reprensenting the \"Manhattan vector\" from\n",
    "# the pickup location to the dropoff location.\n",
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "\n",
    "add_travel_vector_features(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8c6ca0",
   "metadata": {},
   "source": [
    "Are there some undefined values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01248590",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ec78ab",
   "metadata": {},
   "source": [
    "We want to get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb7f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Old size: %d' % len(train_df))\n",
    "train_df = train_df.dropna(how = 'any', axis = 'rows')\n",
    "print('New size: %d' % len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc2cd37",
   "metadata": {},
   "source": [
    "#### Quick analyze on new features and clean outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot = train_df.iloc[:2000].plot.scatter('abs_diff_longitude', 'abs_diff_latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc1a641",
   "metadata": {},
   "source": [
    "70 degrees longitude seems a bit too high..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac95e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Old size: %d' % len(train_df))\n",
    "train_df = train_df[(train_df.abs_diff_longitude < 5.0) & (train_df.abs_diff_latitude < 5.0)]\n",
    "print('New size: %d' % len(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3a4eea",
   "metadata": {},
   "source": [
    "#### Get training features and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca5802",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "# using the travel vector, plus a 1.0 for a constant bias term.\n",
    "def get_input_matrix(df):\n",
    "    return np.column_stack((df.abs_diff_longitude, df.abs_diff_latitude, np.ones(len(df))))\n",
    "\n",
    "train_X = get_input_matrix(train_df)\n",
    "train_y = np.array(train_df['fare_amount'])\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ce60e",
   "metadata": {},
   "source": [
    "#### Train a simple linear model using Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d698bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The lstsq function returns several things, and we only care about the actual weight vector w.\n",
    "(w, _, _, _) = np.linalg.lstsq(train_X, train_y, rcond = None)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b46de57",
   "metadata": {},
   "source": [
    "#### Make prediction on our test set and measure performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab29b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df =  pd.read_csv('gs://obd-dask23/test_cleaned.csv', storage_options={'token': 'anon'})\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_travel_vector_features(test_df)\n",
    "test_X = get_input_matrix(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cf989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_predictions = np.matmul(test_X, w).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_ref = test_df.fare_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f5728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y_ref, test_y_predictions, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c732df4",
   "metadata": {},
   "source": [
    "You should get sabout 6.43 of RMSE, not bad, but we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78fdd59",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "### Some questions on this first Analysis\n",
    "\n",
    "- What is the most expensive part of the analysis, the one that takes the most time (see the %%time we used above)?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea301550",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Textual answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e331b7",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Try to load the whole dataset with Pandas and comment what happens. Can you explain why?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42190bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a22544",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Textual answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478f17c",
   "metadata": {},
   "source": [
    "# Processing our data set using Dask\n",
    "\n",
    "Dask will help us process all the input data set at once. It is really useful when input data is too big to fit in memory. In this case, it can stream the computation by data chunks on one computer, or distribute the computation on several computers.\n",
    "\n",
    "This is what we'll do next!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c22aeb",
   "metadata": {},
   "source": [
    "### Start an appropriately sized Dask cluster for our analysis\n",
    "\n",
    "We'll need a Dask cluster to pre process the data and distribute some learning, the following code starts one in our K8S infrastructure.\n",
    "\n",
    "**Be sure to have any other Dask cluster shutdown.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "gateway = Gateway()\n",
    "gateway.list_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af9392",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = gateway.new_cluster(worker_cores=1, worker_memory=3.0)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a329c1",
   "metadata": {},
   "source": [
    "__Please click on the Dashboard link above, it will help you a lot!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38dac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992aba71",
   "metadata": {},
   "source": [
    "### Launch some computation, what about Pi?\n",
    "\n",
    "Just to check our cluster is working!\n",
    "\n",
    "We'll use Dask array, a Numpy extension for this, we'll also use it later on for the Machine Learning part of this evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893cedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import dask.array as da\n",
    "\n",
    "sample = 10_000_000_000  # <- this is huge!\n",
    "xxyy = da.random.uniform(-1, 1, size=(2, sample))\n",
    "norm = da.linalg.norm(xxyy, axis=0)\n",
    "summ = da.sum(norm <= 1)\n",
    "insiders = summ.compute()\n",
    "pi = 4 * insiders / sample\n",
    "print(\"pi ~= {}\".format(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc0fd7e",
   "metadata": {},
   "source": [
    "## Now, access the data of our BE using Dask\n",
    "\n",
    "We'll use Dask Dataframe, a distributed version of Pandas Dataframe.\n",
    "\n",
    "Remember, Dask shares the same API as Pandas.\n",
    "\n",
    "See https://docs.dask.org/en/latest/dataframe.html.\n",
    "\n",
    "<br>\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "So instead of using Pandas to load the dataset, just use the equivalent dask method from dask.dataframe.\n",
    "\n",
    "- Fill the following cell (the second one) with the appropriate code to read the data using Dask.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2fe6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6905e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c1003",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c5362",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "### Some questions about this data loading\n",
    "\n",
    "- That was fast for several gigabytes, wasn't it? Why is this, did we really load all the data?\n",
    "- Why the returned dataframe looks empty?\n",
    "- See the number of partitions described above? What does it correspond to? (hint, look at the blocksize parameter from https://docs.dask.org/en/latest/generated/dask.dataframe.read_csv.html). You might also get a more precise idea of what this number corresponds to with the next code execution.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfeedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Textual answers needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6834c674",
   "metadata": {},
   "source": [
    "## Little warm up: Analyzing our data to better understand it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc8f8e",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- First, how many records do we have? (hint, in python, len() works for almost any object).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff99939",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1980192c",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- What did happend when counting record of our Dask dataframe (as opposed to with only the `read_csv` call? Remember with the Spark tutorial: transformations and actions... Same kind of concepts exist in Dask. Just look at the Dask Dashboard!\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69982305",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Textual answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668942d8",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Compare the time of this computation to the time of loading a subset of the Dataset with Pandas. Was it fast enough considering the number of workers we have?\n",
    "    \n",
    "I recommend trying to calculate an estimation of the time it would take with Pandas to read the entire dataset, and **total processing** time (not only the walltime) it took for every dask workers.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ac9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code and textual answer needed here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e0f615",
   "metadata": {},
   "source": [
    "Let's have a look at some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba28ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84867e4",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Why was it faster than counting all the records above? \n",
    "- What did we actualy read? You might want to look at the Dashboard for some hinsights. Remember that Dask laziness is not only about transformations and actions, but optimizing the computations needed.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73782ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Textual answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81342f11",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Let's compute the mean of the fare amount given the passengers count, **as we've done with Pandas above**. Please fill the blank. (hint: Dask is the same as pandas, but often with a `compute()` call at the end)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ddf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121f1dbf",
   "metadata": {},
   "source": [
    "Wow, ever seen a cab with more than **200 people**?? Americans are crazy. And it's cheap...\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- This computation is slow, especially compared with Pandas, why? (Look a the Dashboard, again).\n",
    "- Which part of the computation is slow, look at the Dashboard to see the name of the tasks. Hint, this is the same as Pandas.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deca2c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Textual answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff37ce7",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- How could we optimize the next computations, using which Dask method? Same as Spark...\n",
    "- Where will be the data at the end of the computation triggered by this call?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a614e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Textual Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49397217",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Look at the Dashboard at what is happening beind the scene.\n",
    "    \n",
    "Wait for the end of this call on the Dashboard, then try again the previous computation on fare_amout.mean():\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9fa8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here, the same computation on fare_amount mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddadf8d3",
   "metadata": {},
   "source": [
    "Much better isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf8179",
   "metadata": {},
   "source": [
    "## Let's do some preprocessing of our data to clean it up and add some features\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "- You'll need to do the same operations as in pandas, we just need to call compute when needing a result, and not compute when building our dataframe transformations.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adffd39",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "#### Cleaning up\n",
    "\n",
    "- Is there some null values in our data?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbea246",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50fd4c3",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Yep! We must get rid of them...\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff9453",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a1f1b5",
   "metadata": {},
   "source": [
    "#### Adding features\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- As with Pandas above, add the latitude and longitude distance vector with a function call\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a108c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here. We define a function here, cause we'll need to apply it on our test dataframe too later on!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162dafb7",
   "metadata": {},
   "source": [
    "A quick look at our Dataframe to check things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13573f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7cfc5f",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Now let's quickly plot a subset of our travel vector features to see its distribution. Use dask.dataframe.sample() to get about 1 percent of the rows, and get it back with compute and plot as we've done it with Pandas\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f457c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36182399",
   "metadata": {},
   "source": [
    "Wow, looks like we have some strange values here: more than 1000° of distance... There's a problem somewhere.\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "- Just get rid of the extreme values, we should keep only values inside the city wall or so. Like with Pandas above...\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3286a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfc1f4a",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- you can do another plot like above with the filtered values if you like.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45edd5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a525ada",
   "metadata": {},
   "source": [
    "Ok, let's see some statistics on our Dataset. The describe() function inherited from Pandas compute a lot of statistics on a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49070ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7ec27e",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Find some values (at least two) that still looks odd to you in the table above.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Textual answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84739b99",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Do you think we could parallelize things better for any of our computation or data access? (It's a trap).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bfeb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Textual answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead4e91",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "### BONUS Questions (you don't have to do this, just go back to it if you want to improve, skip it at first)\n",
    "\n",
    "Some other questions to practice\n",
    "\n",
    "- Can you see a correlation between the fare amount and the dropoff latitude? Answer by doing a dask dataframe computation.\n",
    "\n",
    "First you'll need to round the dropoff latitude to have some sort of categories using Series.round() function.\n",
    "\n",
    "Then, just group_by this new colon to have some answer (and don't forget to compute to get the results).\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f631f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f28182",
   "metadata": {},
   "source": [
    "OK, this don't give a lot of insights, but it looks like we've got some strange values somewhere!\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "- Let's just have a look of non extreme values, so probably some records at the middle of the results.\n",
    "We need first to sort the resulting series by index before looking at the middle of it.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1430445",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd946db2",
   "metadata": {},
   "source": [
    "OK, this is not really useful, but it's an exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b40adc",
   "metadata": {},
   "source": [
    "## Training a model in a distributed way\n",
    "\n",
    "Let's begin with a linear model that we can distributed with Dask ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e693a",
   "metadata": {},
   "source": [
    "### Building our feature vectors\n",
    "\n",
    "Here again define a method so that we can use it later for our test set evaluation.\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Just do the same as with the Pandas example by defining a get_input_matrix(df) function. But this time you'll generate a dask array (not numpy) using `to_dask_array(lengths=True)` method on the dataframe object instead of `np.column_stack` (look a bit a dask docs in order to find how to use this method). You should do a method that generate the X input features dask array, and also the same with y training results. You can do just one method that return both (return X, y). \n",
    "- It is a good idea to persist() arrays in memory in or after the call.\n",
    "- This time, we'll add the feature 'passenger_count' in addition to the distance vectors, one more feature! So X must have 3 columns.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aad88f",
   "metadata": {},
   "source": [
    "Then we get the values, and display train_X to have some insights of its size and chunking scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c4eea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = get_input_matrix(train_df)\n",
    "# or train_X =\n",
    "# train_y =\n",
    "train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16204e42",
   "metadata": {},
   "source": [
    "### Distributed training a Linear model\n",
    "\n",
    "Be careful, this can take time, try first with few iterations (Use max_iter = 5 as a kwarg to LinearRegression constructor).\n",
    "\n",
    "see https://ml.dask.org/glm.html  \n",
    "and https://ml.dask.org/modules/generated/dask_ml.linear_model.LinearRegression.html#dask_ml.linear_model.LinearRegression\n",
    "\n",
    "<br>\n",
    "\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Train a LinearRegression model from dask_ml.linear_model on our inputs\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed here\n",
    "%%time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ec13f",
   "metadata": {},
   "source": [
    "## Evaluating our model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b3b96f",
   "metadata": {},
   "source": [
    "#### First we should load the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e78929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = dd.read_csv('gs://obd-dask23/test_cleaned.csv', storage_options={'token': 'anon'})\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0695ac",
   "metadata": {},
   "source": [
    "Adding our features to the test set and getting our feature array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2432c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_travel_vector_features(test_df)\n",
    "test_X, test_y = get_input_matrix(test_df)\n",
    "test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a45fa",
   "metadata": {},
   "source": [
    "We can use the score method inherited from Scikit learn, it gives some hints on the model performance (but our scoring board will be on RMSE). Even if for linear models, score if often low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8e4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65be4dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(test_X).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e0b52",
   "metadata": {},
   "source": [
    "#### Compute the RMSE\n",
    "\n",
    "https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y.compute(), lr.predict(test_X).compute(), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618691b3",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- What RMSE did you get? Compare it to the Pandas only computation.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f170c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Textual answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c75934",
   "metadata": {},
   "source": [
    "# Distributed XGboost (optionnal, you can skip it at first)\n",
    "\n",
    "Just use the documentation here https://xgboost.readthedocs.io/en/stable/tutorials/dask.html#overview to train a model on this dataset using xgboost.\n",
    "\n",
    "<br>\n",
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Just copy/paste the example (dtrain = ..., output = ...), and modify some input variables.\n",
    "- Then make a prediction (but don't forget to use your test set, not as in the prediction = ... example from the Xgboost doc).\n",
    "- Compute the mean square error on it.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c158497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580369c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y.compute(), prediction.compute(), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d1443",
   "metadata": {},
   "source": [
    "## Use Dask to scale computation on Hyper Parameter Search\n",
    "\n",
    "As seen above, Dask is well suited to distribute Data and learn a model on a big Data set. However, not all the models can be trained in parallel on sub chunks of Data. See https://scikit-learn.org/stable/computing/scaling_strategies.html for the compatible models of Scikit learn for example.\n",
    "\n",
    "Dask can also be used to train several models in parallel on small datasets, this is what we'll try now.\n",
    "\n",
    "We will just take a sample of the training set, and try to learn several models with different hyper parameters, and find the best one.\n",
    "\n",
    "Dask Hyper parameter search : https://ml.dask.org/hyper-parameter-search.html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc0e7a",
   "metadata": {},
   "source": [
    "First we'll take a small subset of the Data, 5% is a maximum if we want to avoir memory issues on our workers and have appropriate training times. You can try with less if the results are still good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c06652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a sample of the input data, get it as pandas dataframe\n",
    "train_sample_df = train_df.sample(frac=0.05, random_state=270120)\n",
    "# Get feature vectors out of it\n",
    "train_sample_X, train_sample_y = get_input_matrix(train_sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c03fe60",
   "metadata": {},
   "source": [
    "In order to optimize things, we can also change the type of the features to more appropriate and small types.\n",
    "\n",
    "We also need to use Numpy arrays, so we'll gather the result from Dask to local variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee85b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_X = train_sample_X.astype('float32').compute()\n",
    "train_sample_y = train_sample_y.astype('float32').compute()\n",
    "train_sample_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6971e4a",
   "metadata": {},
   "source": [
    "What size is our dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bf7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(train_sample_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f2ee9",
   "metadata": {},
   "source": [
    "About 32MB, this is still quite a big dataset for standard machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1a4fc2",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- Now, just use hyper parameter search Dask API to distribute the search. You can either use joblib integration with Sklearn or dask_ml directly. \n",
    "\n",
    "**Be careful: do not use model too long to train, and limit their complexity at first or the combinations of hyper parameters you'll use. Hint, start first with a simple LinearModel like SGDRegressor and not more than 10 iterations per model.**\n",
    "</span>\n",
    "\n",
    "So start with something like:\n",
    "\n",
    "- RandomizedSearchCV https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html, with cv=2, n_iter=50, verbose=10\n",
    "- With sklearn.linear_model.SGDRegressor with max_iter=20\n",
    "- Use this parameter space:\n",
    "```python\n",
    "from scipy.stats import uniform, loguniform\n",
    "param_space = {\n",
    "    \"average\": [True, False],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    \"alpha\": loguniform(1e-5, 1e-1),\n",
    "    \"learning_rate\": [\"invscaling\", \"adaptive\"],\n",
    "    \"power_t\": uniform(0, 1),\n",
    "}\n",
    "```\n",
    "- If you chose sklearn API, you want to import joblib, and use `with joblib.parallel_backend('dask'):` before fitting your model.\n",
    "- If you chose dask_ml API, https://ml.dask.org/hyper-parameter-search.html#basic-use, you'll don't need the with syntax, but just the correct imports: from dask_ml.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here (with sklearn API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here (with dask_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf1ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, search.predict(test_X), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8545454",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- So how does this result compare to the previous one we got with a distributed leaning with a linear model on all the dataset?\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text answer needed here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661cfe73",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "\n",
    "- Try with https://ml.dask.org/modules/generated/dask_ml.model_selection.HyperbandSearchCV.html#dask_ml.model_selection.HyperbandSearchCV instead of RandomizedSearchCV.\n",
    "    \n",
    "You'd prefer to use dask_ml.model_selection.HyperbandSearchCV (instead of joblib). And just need to change n_iter to max_iter, and remove another arg.\n",
    "    \n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer needed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b489ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d9c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, search.predict(test_X), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9592bec",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- OK, Linear models are what they are, we'll try to do better with Random forest! https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "    \n",
    "Return to RandomizedSearchCV for now.\n",
    "\n",
    "Caution: use limited trees, small number of estimators < 10 and max_depth < 40 at first\n",
    "</span>\n",
    "\n",
    "For example:\n",
    "\n",
    "```python\n",
    "param_space = {\n",
    "'n_estimators': range(4,10),\n",
    "'max_depth': range(10,40),\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc66ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cd8318",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- If you did not used joblib, try it now.\n",
    "\n",
    "What do you observe when training RandomForest tree on Dask parallelization Dashboard with joblib? Can you explain why there are so many tasks?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee3a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Answer here (with sklearn, if not already done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a398ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "search.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72798f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(test_y, search.predict(test_X), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fba8bc",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "- Did you get better results with RandomForest? Do you know why?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39708a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer needed there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83862a8",
   "metadata": {},
   "source": [
    "<span style=\"color:#EB5E0B;font-style:italic\">\n",
    "    \n",
    "# Extend this notebook\n",
    "    \n",
    "With the example aboves, I reached a score of about 4.5 RMSE. Try to do better!\n",
    "\n",
    "- Add new features to the input Data using Dask Dataframes, or clean it better. Reapply the learning above with these new features. Do you get better results? Some suggestions for a better leaning:\n",
    "  - Try to clean extremes or non realistic values you identified above in the training set.\n",
    "  - Apply some normalisation or regularization or other feature transformation? See https://ml.dask.org/preprocessing.html.\n",
    "  - Add some non linear features (square feature, for example square the travel vector)\n",
    "  - Maybe the hour of the day, or the month, has some impact on fares? Try to add features. See https://matthewrocklin.com/blog/work/2017/01/12/dask-dataframes for some hints on how to do this.\n",
    "  - Maybe try to find a way to use the start and drop off locations?\n",
    "- Improve the model parameters or find a better one. Try using this time dask_ml HyperbandSearchCV. See https://ml.dask.org/hyper-parameter-search.html#basic-use. You can use it for example with https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor.\n",
    "- Try one single RandomForestRegressor (with no HyperParameterSearch), but with big depth and estimators. This single model fitting should be distributed on dask with joblib (Random Forest is about training several decision trees).\n",
    "\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d8019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
