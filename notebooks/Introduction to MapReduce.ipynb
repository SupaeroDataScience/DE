{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to MapReduce\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" align=\"left\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png\" /></a>&nbsp;| Dennis G. Wilson | <a href=\"https://supaerodatascience.github.io/DE/\">https://supaerodatascience.github.io/DE/</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using IJulia, Distributed, DataFrames, RDatasets, Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MapReduce programming is the base of many distributed components, notably Apache Hadoop. This programming model is inspired by the `map` and `reduce` functions in many programming languages, such as Julia, which is the focus of this notebook. In this model, processing is split into two steps: map, which applies a function to independent parts of the dataset, and reduce, which aggregates the results from map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MapReduce library, as used by Google and Spark, formalizes this programming model by specifying inputs and outputs of the two functions and by automatically handling data and computation distribution. Specifically, the computation takes a set of input key/value pairs and produces a set of output key/value pairs, which facilitates the distribution and offers a common set of operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the MapReduce Library, `map` takes an input pair and produces a set of intermediate key/value pairs. The MapReduce library groups together all intermediate values associated with the same intermediate key I and passes them to the reduce function. The `reduce` function accepts an intermediate key `I` and a set of values for that key. It merges together these values to form a possibly smaller set of values. Typically just zero or one output value is produced per reduce invocation. The intermediate values are supplied to the user’s reduce function via an iterator. This allows handling lists of values that are too large to fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll focus on understanding the base MapReduce programming model, outside of its formal Spark or Google definition, to get a better understanding of this way of breaking up problems. The first example we'll see is the computation of $\\sum_{i=1}^n i^3$. This simple example won't require a (key, value) map return; single values from map are sufficient for the reduce function here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a map function which we will apply independently to each element of the input array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mapper(x)\n",
    "    x^3\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduce function in this case is addition since we're computing $\\sum_{i=1}^n i^3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function reducer(x, y)\n",
    "    x + y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = map(mapper, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar operator to `map` is `broadcast`, which in Julia can be represented by the `.` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper.(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with the difference between these two operators though, which have different behaviors for multi-dimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map(+, [1,1], [1 2; 3 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast(+, [1,1], [1 2; 3 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have mapped `mapper` to each input, we can reduce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce(reducer, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a shortcut for map and reduce in Julia - `mapreduce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapreduce(mapper, reducer, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also could have used an anonymous function for the mapper and the standard definition of `+` for the reducer, given their simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapreduce(x->x^3, +, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 1 : Approximating π"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo methods are easy to parallelize due to the independent computations performed on random values. In this exercise, we'll calculate an approximation of π using a Monte Carlo method. The principle is simple: given a point defined by $x, y$, where $x$ and $y$ are in $[0, 1]$, we determine if the point is within a quadrant of a unit circle. By randomly generating many such points uniformly between $[0, 1]$, the number of points which falls inside  the quadrant give an approximation to its area. The total area of the space is 1, and the area of the quadrant is $\\frac{π}{4}$, so the number of points inside the quadrant, $q$, divided by the total number of points, $N$, approximates $\\frac{π}{4}$. In other words, $π \\approx 4\\frac{q}{N} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pi_30K.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should define a map and reduce function. What is the purpose of each function? Consider taking in a list of points either as a 2D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = rand(2, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or as a list of coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [rand(2) for i in 1:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mapper(x)\n",
    "    # your code here\n",
    "end\n",
    "\n",
    "function reducer(x, y)\n",
    "    # your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IJulia.load(\"solutions/mapreduce_1.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've defined `mapper` and `reducer`, the following should approximate π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapreduce(mapper, reducer, points) / length(points) * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works but requires us to know the number of points at the end, assuming that the number of points processed is equivalent to the original matrix. One of the advantages of MapReduce is that it allows for flexibility to changing workloads and data sources by putting the necessary information directly into the map and reduce functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewrite your map and reduce functions to include the point count and allow for the mapreduce call to approximate pi independent of the length of points. Your final result should be a tuple of $(q, N)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IJulia.load(\"solutions/mapreduce_2.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, N = mapreduce(mapper, reducer, points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q / N * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This representation is flexible to combining multiple responses from different simulations, allowing for workers with different loads to be able to contribute to the computation. To demonstrate this, we'll use the parallel processing library `Distributed`. We can add two additional threads to the notebook's worker pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed\n",
    "addprocs(2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `@distributed` macro, we can launch parallel computations. However, to do this, we need to define our mapper and reducer functions on all threads using the `@everywhere` macro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere function mapper(x)\n",
    "    # your code here\n",
    "end\n",
    "\n",
    "@everywhere function reducer(x, y)\n",
    "    # your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, we can apply a reduction operator to the result of each of these threads. We will reuse the previously defined reduce operator to combine the results from independent `reduce` calls. This idea of chaining map or reduce functions is a common design in MapReduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, N = @distributed reducer for i in 1:10\n",
    "    points = [rand(2) for j in 1:rand(100:5000)]\n",
    "    mapreduce(mapper, reducer, points)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q / N * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 2 : Average of a List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider that we have a long list of floating point values and we want to determine the average of this list. To speed it up, it would be best to split the computation over multiple different threads, meaning splitting up the list. Write map and reduce functions for this average calculating, making sure that your functions will work even if the full list isn't split evenly between workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "μ = 0.345\n",
    "list = randn(100000) .+ μ\n",
    "nworkers = 5\n",
    "s = [0]\n",
    "append!(s, sort!(rand(1:length(list), nworkers-1)))\n",
    "push!(s, length(list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your map function `av_mapper` and reduce function `av_reducer`. How can you combine the results from different parts of the list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IJulia.load(\"solutions/mapreduce_3.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = @distributed av_reducer for i in 1:nworkers\n",
    "    chunk = list[s[i]+1:s[i+1]]\n",
    "    println(\"processing \", length(chunk), \" samples\")\n",
    "    mapreduce(av_mapper, av_reducer, chunk)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can double-check your results using the standard `mean` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study 3: Grep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll implement a similar command to the Unix command `grep`, allowing us to search through a text for a specific pattern. This could be used to search through data logs, text documents, or full datasets. We'll aim to output just how many time each search pattern appears in the document, not its line number as grep does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text we'll use is Alice's Adventures in Wonderland. You can download a copy rather quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(`wget \"http://www.umich.edu/~umfandsf/other/ebooks/alice30.txt\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = readlines(\"alice30.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to match regular expressions in each line of text, we'll use the `match` or `eachmatch` function in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = eachmatch(r\".l.ce\", lines[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[(r.match, 1) for r in collect(matches)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, look for one of these three regular expressions. Note that `\".abbit\"` can match \"rabbit\", \"Rabbit\", or any other word ending in \"abbit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = [r\"Alice\", r\".abbit\", r\"Queen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write map and reduce functions to parse each line of the text and count the instances of the above regular expressions. You may find the [Hadoop version](https://cwiki.apache.org/confluence/display/HADOOP2/Grep) of grep helpful. Try getting it to work on a single thread before considering how to distribute the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Bonus exercise</b>\n",
    "    \n",
    "Adapt your MapReduce version of grep to find out the distribution of word lengths in Alices Adventure's in Wonderland and plot it as a histogram.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split-Apply-Combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MapReduce programming model is a case of a programming design very common in data processing tasks, whether benefitting from distributed computation or not. This design is known as [\"split-apply-combine\"](http://www.jstatsoft.org/v40/i01) and is based around splitting a data set into groups, applying some functions to each of the groups and then combining the results. We'll see an example of this on the `iris` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = dataset(\"datasets\", \"iris\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this paradigm is to split the data, often based on the task at hand. If you're trying to study a specific feature, split the data on this feature, for example. However, split can also be used as in most MapReduce functions for data distribution and increased performance. Here, we'll look at splitting the dataset by Species to gather statistics about this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = groupby(iris, :Species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine(gdf, :PetalLength => mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine(gdf) do df\n",
    "    (m = mean(df.PetalLength), s² = var(df.PetalLength), min = minimum(df.PetalLength), max = maximum(df.PetalLength))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Discussion</b>\n",
    "    \n",
    "Consider the above examples. First, identify how the dataset was split, what functions were applied, and how the results were combined. Considering this seperation, what is the `map` function equivalent? What is the equivalent of `reduce`?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further reading, this [blogpost](https://towardsdatascience.com/how-to-use-the-split-apply-combine-strategy-in-pandas-groupby-29e0eb44b62e) gives a good overview of applying this framework in pandas through the `groupby-apply` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
