{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Data Engineering","text":"<p>The amount of data in the world, the form these data take, and the ways to interact with data have all increased exponentially in recent years. The extraction of useful knowledge from data has long been one of the grand challenges of computer science, and the dawn of \"big data\" has transformed the landscape of data storage, manipulation, and analysis. In this module, we will look at the tools used to store and interact with data.</p> <p>The objective of this class is that students gain:</p> <ul> <li>First hand experience with and detailed knowledge of computing models, notably cloud computing</li> <li>An understanding of distributed programming models and data distribution</li> <li>Broad knowledge of many databases and their respective strengths</li> </ul> <p>As a part of the Data and Decision Sciences Master's program, this module aims specifically at providing the tool set students will use for data analysis and knowledge extraction using skills acquired in the Algorithms of Machine Learning and Digital Economy and Data Uses classes.</p>"},{"location":"index.html#class-structure","title":"Class structure","text":"<p>The class is structured in three parts:</p>"},{"location":"index.html#data-storage","title":"Data storage","text":"<p>In the first 10 hours of the course, the history of data storage from single   databased management systems to distributed filesystems will be presented. For   evaluation, students will install and manipulate data in PostgreSQL.</p>"},{"location":"index.html#data-computation","title":"Data computation","text":"<p>20 hours on the computing platforms used in the data ecosystem. We will   briefly cover cluster computing and then go in depth on cloud computing, using   Google Cloud Platform as an example. Finally, a class on GPU computing will be   given in coordination with the deep learning section of the AML class.</p>"},{"location":"index.html#data-distribution","title":"Data distribution","text":"<p>20 hours on the distribution of data, with a focus on distributed programming   models. We will introduce functional programming and MapReduce, then use these   concepts in a practical session on Spark. Finally, students will do a graded   exercise with Dask.</p>"},{"location":"index.html#class-schedule","title":"Class schedule","text":"Data Storage Readings SQL 3h 18/09/2023 Databases introduction (fr) PostgeSQL 3h 25/09/2023 PostgeSQL Parallel DBMS 4h 04/10/2023 Data Computation Readings Cloud Computing 3h 21/11/2023 Readings Containers 3h 28/11/2023 Readings Cloud Compute BE 3h 29/11/2023 Distributed DBMS 3h 04/12/2023 GPU computing 6h 13/12/2023 Exam 2h 19/12/2023 Data Distribution Readings Orchestration 3h 08/01/2024 Readings Deployment TP 3h 09/01/2024 Readings Hadoop and MapReduce 3h 16/01/2024 MapReduce Spark 4h 17/01/2024 Spark PySpark Cloud DBMS 3h 04/12/2023 Dask 3h 13/02/2024 Dask documentation Dask project 3h 13/02/2024 Dask"},{"location":"0_1_databases.html","title":"Data Storage","text":"<p>In this module on databases, database management systems will be covered. A basic understanding of SQL is considered as a prerequisite, and students can refer to the slides and additional resources if needed. For evaluation, students will install and explore the advantages of different DBMSs as a graded project.</p> <p>In this first class, we introduce the basics of database management systems and cover high level DBMS functionality.</p> <p>Slides</p> <p>For the next class, students should install PostgreSQL and MongoDB on their local machines.</p>"},{"location":"0_1_databases.html#additional-resources","title":"Additional Resources","text":"<ul> <li>PostgreSQL documentation</li> <li>MongoDB documentation</li> <li>SQLBolt - SQL exercises</li> <li>Databases introduction (fr)</li> <li>A comprehensive overview of database systems (en)</li> </ul>"},{"location":"0_2_postgres.html","title":"PostgeSQL","text":"<p>In this practical session, we cover many examples of database queries with the popular DBMS PostgreSQL.</p> <p>Based on the TP by Christophe Garion, CC BY-NC-SA 2015.</p>"},{"location":"0_2_postgres.html#setup","title":"Setup","text":"<p>Before class, please install PostgreSQL and pgAdmin.</p>"},{"location":"0_2_postgres.html#postgresql-installation","title":"PostgreSQL installation","text":"<p>For this session, students should install PostgreSQL (v9 or higher) and pgAdmin (v4). Follow the installation instructions and make sure you have an initial database setup and the <code>postgresql</code> service running.</p> <ul> <li>Installation on Ubuntu</li> <li>Installation on Mac OS</li> <li>Installation on Arch Linux</li> <li>Installation on Windows Subsystem for Linux</li> <li>Installation on Windows (and add the PostgreSQL binaries to your path)</li> </ul> <p>Additionally, add your login user as a postgresql superuser to enable database creation with your user:</p> <pre><code># bash shell in Linux or OSX\n$ sudo su -l postgres\n[postgres]$ createuser --interactive\n</code></pre>"},{"location":"0_2_postgres.html#pgadmin","title":"pgAdmin","text":"<p>You can do all exercises directly through the <code>psql</code> shell for this class. However, it is useful to have a graphical confirmation of the database configuration. pgAdmin is one of many front-ends for Postgres. Install it by following the instructions on the pgAdmin site.</p>"},{"location":"0_2_postgres.html#setup-database-creation","title":"Setup - database creation","text":"<p>Once you've installated and configured PostgreSQL, create the first exercise database:</p> <pre><code># bash shell in Linux or OSX or windows powershell\n$ createdb db-mexico86\n</code></pre> <p>you can also do this through an SQL shell:</p> <pre><code># SQL shell\npostgres=# CREATE DATABASE \"db-mexico86\";\n</code></pre> <p>Confirm with pgAdmin that your database <code>db-mexico86</code> was created. If you don't have any servers, create one by right-clicking. The host address is <code>127.0.0.1</code> and the maintenance database and username should be <code>postgres</code>.</p> <p>In pgAdmin, if you are asked for a password and don't know what your password is, you can reset the password of the postgres user:</p> change password <pre><code>postgres=# ALTER USER postgres WITH PASSWORD \"newpassword\";\n</code></pre>"},{"location":"0_2_postgres.html#mexico86-database-simple-queries","title":"Mexico86 database - simple queries","text":"<p>This database contains data from the 1986 football World Cup. </p> <p>You can download the database creation script individually: <pre><code>$ wget https://raw.githubusercontent.com/SupaeroDataScience/DE/master/scripts/mexico86/create-tables-std.sql\n</code></pre></p> <p>Or git clone the class repository and navigate to the creation and insertion scripts.</p> <p>Once you have the scripts, run the database creation script in the <code>mexico</code> folder.</p> <pre><code># bash shell in Linux or OSX, or windows powershell\n$ psql -d db-mexico86 -f mexico86/create-tables-std.sql\n</code></pre> <p>If that doesn't work, you can copy the script into the Query Tool in pgAdmin.</p> <p>Exercise 1.1: Look at the database creation scripts. What are the tables being created? What are their fields? Which fields are keys? Confirm these values in pgAdmin.</p> Response  Pays: (nom, groupe)  Typematch: (type)  Match: (paysl, paysv, butsl, butsv, type, date)   <p>You should be able to make queries now. You can either use PostgreSQL in interactive mode by running </p> <pre><code>$ psql -d db-mexico86\n</code></pre> <p>or write your solutions in an SQL file and run the file:</p> <pre><code>$ echo \"SELECT groupe FROM pays;\" &gt; a.sql\n$ psql -d db-mexico86 -f a.sql\n</code></pre> <p>You can also use the Query Editor in pgAdmin for a graphical interface.</p> <p>Exercise 1.2: Write a query which lists the countries participating in the World Cup.</p> Response <pre><code>        nom \n---------------------\nArgentine\nItalie\nBulgarie\nCor\u00e9e\nMexique\nParaguay\nBelgique\nIrak\nURSS\nHongrie\nFrance\nCanada\nBr\u00e9sil\nEspagne\nIrlande du Nord\nAlg\u00e9rie\nDanemark\nRFA\nUruguay\n\u00c9cosse\nMaroc\nAngleterre\nPologne\nPortugal\n(24 rows)\n</code></pre> <p>Exercise 1.3: Write a query which lists all matches as a pair of countries per match.</p> Response <pre><code>        paysl        |        paysv \n---------------------|---------------------\nBulgarie            | Italie\nArgentine           | Cor\u00e9e\nItalie              | Argentine\nCor\u00e9e               | Bulgarie\nCor\u00e9e               | Italie\nArgentine           | Bulgarie\nBelgique            | Mexique\nParaguay            | Irak\nMexique             | Paraguay\nIrak                | Belgique\nIrak                | Mexique\nParaguay            | Belgique\nCanada              | France\nURSS                | Hongrie\nFrance              | URSS\nHongrie             | Canada\nURSS                | Canada\nHongrie             | France\nEspagne             | Br\u00e9sil\nAlg\u00e9rie             | Irlande du Nord\nBr\u00e9sil              | Alg\u00e9rie\nIrlande du Nord     | Espagne\nIrlande du Nord     | Br\u00e9sil\nAlg\u00e9rie             | Espagne\nUruguay             | RFA\n\u00c9cosse              | Danemark\nDanemark            | Uruguay\nRFA                 | \u00c9cosse\n\u00c9cosse              | Uruguay\nDanemark            | RFA\nMaroc               | Pologne\nPortugal            | Angleterre\nAngleterre          | Maroc\nPologne             | Portugal\nAngleterre          | Pologne\nMaroc               | Portugal\nBr\u00e9sil              | Pologne\nFrance              | Italie\nMaroc               | RFA\nMexique             | Bulgarie\nArgentine           | Uruguay\nAngleterre          | Paraguay\nURSS                | Belgique\nEspagne             | Danemark\nBr\u00e9sil              | France\nRFA                 | Mexique\nArgentine           | Angleterre\nBelgique            | Espagne\nFrance              | RFA\nArgentine           | Belgique\nRFA                 | Argentine\n(51 rows)\n</code></pre> <p>Exercise 1.4: Write a query which lists the matches which took place on June 5, 1986.</p> Response <pre><code>        paysl        |   paysv\n---------------------|-----------\nItalie              | Argentine\nCor\u00e9e               | Bulgarie\nFrance              | URSS\n(3 rows)\n</code></pre> <p>Exercise 1.5: Write a query which lists the countries which France played against (hint, France could have played either side).</p> Response <pre><code>pays\n---------\nBr\u00e9sil\nCanada\nHongrie\nItalie\nRFA\nURSS\n(6 rows)\n</code></pre> <p>Exercise 1.6: Write a query which returns the winner of the World Cup</p> Response <pre><code>pays\n-----------\nArgentine\n(1 row)\n</code></pre>"},{"location":"0_2_postgres.html#beer-database","title":"Beer database","text":"<p>We'll now use a database which tracks the beers that a group of friends enjoy. Create the database and populate it using the provided scripts.</p> <pre><code>$ createdb db-beer\n$ psql -d db-beer -f beer/create-tables-std.sql\n$ psql -d db-beer -f beer/insert.sql\n</code></pre> <p>Exercise 2.1: Look at the database creation scripts. What are the tables being created? What are their fields? Which fields are keys? Confirm these values in pgAdmin.</p> Response  Frequente: (buveur, bar)  Sert: (bar, biere)  Aime: (buveur, biere)   <p>Write queries which respond to the following questions. Hint, understanding natural joins may help.</p> <p>Exercise 2.2 What is the list of bars which serve the beer that Martin likes?</p> Response <pre><code>        bar \n-------------------\n Ancienne Belgique\n La Tireuse\n Le Filochard\n(3 rows)\n</code></pre> <p>Exercise 2.3 What is the list of drinkers who go to at least one bar which servers a beer they like?</p> Response <pre><code> buveur \n--------\n Bob\n David\n Emilie\n Martin\n(4 rows)\n</code></pre> <p>Exercise 2.3 What is the list of drinkers who don't go to any bars which serve the beer they like?</p> Response <pre><code> buveur \n--------\n Cecile\n Alice\n(2 rows)\n</code></pre>"},{"location":"0_2_postgres.html#complex-queries-mexico-database","title":"Complex queries - Mexico database","text":"<p>Exercise 3.1: Create a table with an entry for each match which lists the total number of goals (scored by either side), the match type, and the date. As we'll use this table later on, create a VIEW called \"matchbutsglobal\" with this information.</p> Response <pre><code>        paysl        |        paysv        | buts |  type  |    date \n---------------------+---------------------+------+--------+------------\n URSS                | Belgique            |    7 | 1/8    | 1986-06-15\n France              | Italie              |    2 | 1/8    | 1986-06-17\n Maroc               | Pologne             |    0 | Poule  | 1986-06-02\n RFA                 | Argentine           |    5 | Finale | 1986-06-29\n Br\u00e9sil              | France              |    2 | 1/4    | 1986-06-21\n Italie              | Argentine           |    2 | Poule  | 1986-06-05\n Maroc               | Portugal            |    4 | Poule  | 1986-06-11\n Br\u00e9sil              | Alg\u00e9rie             |    1 | Poule  | 1986-06-06\n Paraguay            | Belgique            |    4 | Poule  | 1986-06-11\n Hongrie             | France              |    3 | Poule  | 1986-06-09\n Irak                | Belgique            |    3 | Poule  | 1986-06-08\n Danemark            | RFA                 |    2 | Poule  | 1986-06-13\n Irlande du Nord     | Espagne             |    3 | Poule  | 1986-06-07\n Alg\u00e9rie             | Irlande du Nord     |    2 | Poule  | 1986-06-03\n RFA                 | Mexique             |    0 | 1/4    | 1986-06-21\n URSS                | Hongrie             |    6 | Poule  | 1986-06-02\n Mexique             | Paraguay            |    2 | Poule  | 1986-06-07\n Belgique            | Espagne             |    2 | 1/4    | 1986-06-22\n Irak                | Mexique             |    1 | Poule  | 1986-06-11\n Espagne             | Br\u00e9sil              |    1 | Poule  | 1986-06-01\n Angleterre          | Maroc               |    0 | Poule  | 1986-06-06\n Irlande du Nord     | Br\u00e9sil              |    2 | Poule  | 1986-06-12\n Maroc               | RFA                 |    1 | 1/8    | 1986-06-17\n Belgique            | Mexique             |    3 | Poule  | 1986-06-03\n Bulgarie            | Italie              |    2 | Poule  | 1986-05-31\n \u00c9cosse              | Uruguay             |    0 | Poule  | 1986-06-13\n Alg\u00e9rie             | Espagne             |    3 | Poule  | 1986-06-12\n Argentine           | Belgique            |    2 | 1/2    | 1986-06-25\n Br\u00e9sil              | Pologne             |    4 | 1/8    | 1986-06-16\n Danemark            | Uruguay             |    7 | Poule  | 1986-06-08\n Cor\u00e9e               | Italie              |    5 | Poule  | 1986-06-10\n Canada              | France              |    1 | Poule  | 1986-06-01\n Argentine           | Uruguay             |    1 | 1/8    | 1986-06-16\n France              | RFA                 |    2 | 1/2    | 1986-06-25\n France              | URSS                |    2 | Poule  | 1986-06-05\n Uruguay             | RFA                 |    2 | Poule  | 1986-06-04\n Angleterre          | Pologne             |    3 | Poule  | 1986-06-11\n Portugal            | Angleterre          |    1 | Poule  | 1986-06-03\n \u00c9cosse              | Danemark            |    1 | Poule  | 1986-06-04\n Angleterre          | Paraguay            |    3 | 1/8    | 1986-06-18\n Hongrie             | Canada              |    2 | Poule  | 1986-06-06\n Argentine           | Cor\u00e9e               |    4 | Poule  | 1986-06-02\n Pologne             | Portugal            |    1 | Poule  | 1986-06-07\n RFA                 | \u00c9cosse              |    3 | Poule  | 1986-06-08\n Mexique             | Bulgarie            |    2 | 1/8    | 1986-06-15\n URSS                | Canada              |    2 | Poule  | 1986-06-09\n Espagne             | Danemark            |    6 | 1/8    | 1986-06-18\n Paraguay            | Irak                |    1 | Poule  | 1986-06-04\n Argentine           | Bulgarie            |    2 | Poule  | 1986-06-10\n Argentine           | Angleterre          |    3 | 1/4    | 1986-06-22\n Cor\u00e9e               | Bulgarie            |    2 | Poule  | 1986-06-05\n(51 rows)\n</code></pre> <p>Exercise 3.2: Write a query which caluculates the number of goals scored on average in all the matches of the French team.</p> Response <pre><code>    Moyenne buts\n--------------------\n 2.0000000000000000\n(1 row)\n</code></pre> <p>Exercise 3.3: Write a query which calculates the total number of goals scored only by the French team.</p> Response <pre><code> buts \n------\n    8\n(1 row)\n</code></pre> <p>Exercise 3.4: Write a query which caluclates the total number of goals scored in each Poule match. Order the results by group.</p> Response <pre><code> groupe | sum \n--------+-----\n A      |  17\n B      |  14\n C      |  16\n D      |  12\n E      |  15\n F      |   9\n(6 rows)\n</code></pre> <p>Exercise 3.5: Write a function <code>vainquer</code> which takes in the two countries of a match and the match type and which returns the winner. Apply your function to the following pairs:</p> <pre><code>SELECT * FROM vainqueur('Espagne', 'Danemark', '1/8');\nSELECT * FROM vainqueur('Br\u00e9sil', 'France', '1/4');\n</code></pre> Response <pre><code> vainqueur \n-----------\n Espagne\n(1 row)\n\n vainqueur \n-----------\n Match nul\n(1 row)\n</code></pre> <p>Exercise 3.6: Write a function <code>butsparequipe</code> which returns the total and the average number of points scored by a team. Apply your function to the French team. Bonus points for making the result display the name of the team.</p> <pre><code>SELECT * FROM butsparequipe('France');\n</code></pre> Response <pre><code>  pays  | total |      moyenne \n--------+-------+--------------------\n France |     8 | 1.3333333333333333\n(1 row)\n</code></pre> <p>Exercise 3.7: Using the <code>butsparequipe</code> function, write a query which lists all countries and the points they scored. </p> Response <pre><code>        pays         | total \n---------------------+-------\n Argentine           |    14\n Italie              |     5\n Bulgarie            |     2\n Cor\u00e9e               |     4\n Mexique             |     6\n Paraguay            |     4\n Belgique            |    10\n Irak                |     1\n URSS                |    12\n Hongrie             |     2\n France              |     8\n Canada              |     0\n Br\u00e9sil              |     9\n Espagne             |    11\n Irlande du Nord     |     2\n Alg\u00e9rie             |     1\n Danemark            |    10\n RFA                 |     8\n Uruguay             |     2\n \u00c9cosse              |     1\n Maroc               |     3\n Angleterre          |     7\n Pologne             |     1\n Portugal            |     2\n(24 rows)\n</code></pre> <p>Exercise 3.8: Using the <code>butsparequipe</code> function, write a query which shows the country which scored the most points and the number of points they scored.</p> Response <pre><code>   pays    | total \n-----------+-------\n Argentine |    14\n(1 row)\n</code></pre>"},{"location":"0_2_postgres.html#pull-the-trigger","title":"Pull the trigger","text":"<p>In this exercise, we're going to create a TRIGGER, a mechanism which allows for automatically executing actions when an event occurs.</p> <p>Create the <code>db-trigger</code> database.</p> <pre><code>$ createdb db-trigger\n</code></pre> <p>Exercise 4.1: Create a table <code>rel(nom, value)</code> where <code>nom</code> is a string of characters and <code>value</code> is an integer. <code>nom</code> will be the primary key</p> Solution <pre><code>CREATE TABLE IF NOT EXISTS rel (\n    nom VARCHAR(20),\n    valeur INTEGER,\n    PRIMARY KEY (nom)\n);\n</code></pre> <p>Exercise 4.2: Add 5 tuples into the table</p> Solution <pre><code>INSERT INTO rel VALUES\n       ('Alice', 10),\n       ('Bob', 5),\n       ('Carl', 20),\n       ('Denise', 11),\n       ('Esther', 6);\n</code></pre> <p>Exercise 4.3: Write a trigger such that, when adding new tuples, the average value of <code>val</code> cannot decrease. If a new tuple is added which would decrease the average, an exception should be raised.</p> <p>The following insertion should work:</p> <pre><code>INSERT INTO rel VALUES ('Fab', 15);\n\nSELECT * FROM rel;\n</code></pre> <p>As we can see, the <code>(Fab, 15)</code> tuple was added:</p> <pre><code>  nom   | valeur \n--------+--------\n Alice  |     10\n Bob    |      5\n Carl   |     20\n Denise |     11\n Esther |      6\n Fab    |     15\n(6 rows)\n</code></pre> <p>However, the following insertion should give an exception:</p> <pre><code>INSERT INTO rel VALUES ('Guy', 2);\n</code></pre> Solution <pre><code>CREATE OR REPLACE FUNCTION verifier_moyenne()\n                  RETURNS trigger AS $verifier_moyenne$\n    DECLARE\n      moyenne FLOAT;\n      nb      INTEGER;\n    BEGIN\n        moyenne := AVG(valeur) FROM rel;\n        nb := COUNT(*) FROM rel;\n\n        IF ((nb * moyenne + NEW.valeur) / (nb + 1)) &lt; moyenne THEN\n            RAISE EXCEPTION 'problem with insertion: valeur average is decreasing!';\n        END IF;\n\n        RETURN NEW;\n    END;\n$verifier_moyenne$ LANGUAGE plpgsql;\n\nCREATE TRIGGER VerificationMoyenne\nBEFORE INSERT ON rel\nFOR EACH ROW\nEXECUTE PROCEDURE verifier_moyenne();\n</code></pre>"},{"location":"0_2_postgres.html#deliverables","title":"Deliverables","text":"<p>For evaluation, you should provide a single PostgreSQL script which reproduces the results of all exercises. You should upload the script to the LMS by October 3.</p>"},{"location":"0_3_dbms.html","title":"Evolution of Data Management Systems","text":""},{"location":"0_3_dbms.html#fundamental-concepts-methods-and-applications","title":"Fundamental Concepts, Methods and Applications","text":"<p>In this three part class, students will cover the history of data management systems, from file systems to databases to distributed cloud storage. This class is given over the length of the Data Engineering course. Questions from the first two parts are integrated into the exam on cloud computing, and questions from the Cloud DMS section are integrated into the Dask notebook evaluation.</p>"},{"location":"0_3_dbms.html#objectives","title":"Objectives","text":"<p>The objectives of this course are: - Introduce the fundamental concepts - Describe, in a synthetic way, the main characteristics of the evolution of DMS (Data Management Systems) - Highlight targeted application classes.</p>"},{"location":"0_3_dbms.html#key-words","title":"Key Words","text":"<p>Data Management Systems, Uni-processor DBMS, Parallel DBMS, Data Integration Systems,Big Data, Cloud  Data Management Systems, High Performance, Scalability, Elasticity, Multi-store/Poly-store Systems</p>"},{"location":"0_3_dbms.html#targeted-skills","title":"Targeted Skills","text":"<ul> <li>Effectively exploit the DMS according to the environment (uniprocessor, parallel, distributed, cloud) in a perspective of decision support within an organization.</li> <li>Ability to choose, in a relevant way, a DMS in multiple environments for an optimal functioning of the applications of an organization</li> </ul>"},{"location":"0_3_dbms.html#indicative-program","title":"Indicative Program","text":"<ol> <li> <p>Introduction to Main Problems of Data Management</p> <ul> <li>From File Management Systems FMS to Database MS DBMS</li> <li>Motivations, Objectives, Organizations &amp; Drawbacks</li> <li>Databases &amp; Rel. DBMS: Motivations &amp; Objectives</li> <li>Resources:<ul> <li>Introduction</li> <li>SGF - File Systems</li> <li>Views - Relational Systems</li> <li>File Organization</li> </ul> </li> </ul> </li> <li> <p>Parallel Database Systems</p> <ul> <li>Objectives and Parallel Architecture Models</li> <li>Data Partitioning Strategies</li> <li>Parallel Query Processing</li> <li>Motivations &amp; Objectives</li> <li>Characteristics and Challenges</li> <li>Resources:<ul> <li>Parallel DBMS</li> <li>Parallel Queries</li> </ul> </li> </ul> </li> <li> <p>From Distributed DB to Data Integration Systems DIS</p> <ul> <li>An Ex. of DBD, Motivations &amp; Objectives</li> <li>Designing of DDB</li> <li>Distributed Query Processing</li> <li>An Ex. of DIS</li> <li>Motivations &amp; Objectives</li> <li>Mediator-Adapters Architecture</li> <li>Design of a Global Schema (GAV, LAV)</li> <li>Query Processing Methodologies</li> </ul> </li> <li> <p>Cloud Data Management Systems CDMS</p> <ul> <li>Motivations and Objectives</li> <li>Main Characteristics of Big Data and CDMS</li> <li>Classification of Cloud Data Management Systems CDMS</li> <li>Advantages and Weakness of Parallel RDBMS and CDMS</li> <li>Comparison between Parallel RDBMS and CDMS</li> <li>Introduction to Multi-store/Ploystore Systems</li> <li>Resources:<ul> <li>Distributed DBMS - Chapter 1</li> <li>Distributed DBMS - Chapter 2</li> <li>Distributed DBMS - Chapter 3</li> <li>Systems for integrating heterogeneous and distributed data</li> <li>Integration Systems complement</li> <li>MapReduce examples</li> </ul> </li> </ul> </li> <li> <p>Conclusion</p> <ul> <li>Maturity of Cloud DMS</li> <li>Key Criteria for Choosing a Data Management System</li> </ul> </li> </ol>"},{"location":"0_3_dbms.html#additional-reading","title":"Additional Reading","text":"<ol> <li> <p>Principles of Distributed Database Systems,  M. Tamer Ozsu  and Patrick Valduriez; Springer-Verlag ;  Fourth Edition,  December 2019.</p> </li> <li> <p>Data Management in the Cloud: Challenges and Opportunities Divyakant Agrawal, Sudipto Das, and Amr El Abbadi; Synthesis Lectures on Data Management, December 2012, Vol. 4, No. 6 , Pages 1-138.</p> </li> <li> <p>Query Processing in Parallel Relational Database Systems; H. Lu, B.-C Ooi and K.-L. Tan; IEEE Computer Society Press, CA, USA, 1994.</p> </li> <li> <p>Traitement parall\u00e8le dans les bases de donn\u00e9es relationnelles : concepts, m\u00e9thodes et applications Abdelkader Hameurlain, Pierre Bazex, Franck Morvan; C\u00e9padu\u00e8s Editions,  Octobre 1996.  </p> </li> </ol>"},{"location":"0_3_project.html","title":"NoSQL Databases Project","text":"<p>The evaluation of the databases class is a presentation of a specific DBMS. You can work in teams of 4 of your choosing.</p> <p>The idea of this project is to study new DBMSs, with a focus on NoSQL DBMSs. There is a presentation on the differences between relational databases and various NoSQL DBMSs and another here on the history of NoSQL. In this project, students should compare their DBMS with a relational DBMS (PostgreSQL) to understand the advantages and disadvantages of various NoSQL DBMSs.</p> <p>You are working in a company which is looking to replace a relational DBMS currently in use. Each team should present a feasability study of a specific DBMS, showing its advantages, disadvantages, and use cases. We will organize the subjects on 06/10/2021, which is a work class dedicated to the project. Up to two different teams can work on each DBMS.</p> <p>The possible DBMSs are:</p> <ol> <li>MongoDB, a DBMS for documents used, for example, by CERN</li> <li>Cassandra, a distributed data storage system for handling very large amounts of structured data</li> <li>Redis, a very efficient key/value DBMS</li> <li>HBase, a distributed and non-relationed column-based DBMS</li> <li>Neo4j, a native graph DBMS</li> <li>RavenDB, a document DBMS with ACID integrity</li> <li>Couchbase, a document DBMS for interactive web applications</li> <li>CouchDB, a JSON-based DBMS with native JavaScript support</li> <li>InfluxDB, a distributed DBMS optimized for timeseries data</li> <li>OrientDB, a DMBS for graph data</li> </ol> <p>Each team should:</p> <ul> <li>install their DBMS</li> <li>test the DBMS on a relevant database (datasets from Google, and kaggle)</li> <li>compare their DBMS with a relational database system</li> <li>prepare a presentation of their DBMS and example database which presents convincing argument for using this DBMS</li> <li>evaluate how ACID or BASE principles are met by their DBMS</li> </ul> <p>A good example from previous years is here. This was in the form of a report, but currently a report is not required, just the oral presentation.</p> <p>Presentations will take place on 02/11/2021. You should upload your presentation materials to the LMS before midnight on 01/11/2021.</p>"},{"location":"1_1_overview.html","title":"Data Computation Part 1: Cloud Computing, Containers &amp; Deployment","text":"<ul> <li>Back to home</li> </ul>"},{"location":"1_1_overview.html#introduction","title":"Introduction","text":"<p>Introduction to data computation module</p>"},{"location":"1_1_overview.html#quiz-and-recap","title":"Quiz and recap","text":"<p>The evaluation of this section will be done with an open-resource quiz covering all cloud computing topics. These concluding slides should be used to recap the previous courses.</p>"},{"location":"1_1_syllabus.html","title":"Data Computation Part 1: Cloud Computing, Containers &amp; Deployment","text":"<ul> <li>Back to home</li> <li>All slides</li> </ul>"},{"location":"1_1_syllabus.html#syllabus","title":"Syllabus","text":""},{"location":"1_1_syllabus.html#introduction","title":"Introduction","text":"<p>Introduction to data computation module</p>"},{"location":"1_1_syllabus.html#cloud-computing-3h","title":"Cloud Computing (3h)","text":"<p>Intro to cloud computing &amp; google cloud platform</p> Date Type Link Description 23/01 Lecture Intro to Cloud Computing A lecture about an introduction to \"what is the cloud\" 23/01 Lecture Using Cloud Computing in your daily job What does it mean to \"use the cloud\" ? 23/01 Lecture Intro to Google Cloud Platform A quick intro to GCP 23/01 Hands-on GCP : Setup, Hands On, My first VMs, Storage, IAAS &amp; PAAS My first steps with GCP, Google Cloud Shell, discovering GCE &amp; GCS 13/02 Recap Important notions about Cloud Computing A recap of important notions"},{"location":"1_1_syllabus.html#containers-3h","title":"Containers (3h)","text":"<p>Intro to containers &amp; docker</p> Date Type Link Description 24/01 Lecture From virtualisation to Containerisation, Docker What are containers and why do we need them ? What is Docker ? 24/01 Hands-on Docker Discover Docker 13/02 Recap Important notions about Docker A recap of important notions"},{"location":"1_1_syllabus.html#be-3h","title":"BE (3h)","text":"<p>A small workshop that puts everything together: Google cloud &amp; docker</p> Date Type Link Description 31/01 BE Explanations Walkthrough A final Bureau d'\u00e9tudes to wrap everything together"},{"location":"1_1_syllabus.html#deployment-3h","title":"Deployment (3h)","text":"<p>Deploy your machine learning model in production with everything you've learnt</p> Date Type Link Description 06/02 Lecture MLOPS and Deployement Learn about webservices, microservices, restful apis, docker compose, container orchestration and kubernetes 06/02 Interactive Deploying ML models into production Deploy your ML model ! 06/02 Lecture Orchestration Learn about webservices, microservices, restful apis, docker compose, container orchestration and kubernetes At Home Bonus Hands-on My first kubernetes cluster for a development environment Where we deploy a development environment on Kubernetes 13/02 Recap Important notions about Orchestration A recap of important notions"},{"location":"1_1_syllabus.html#quiz-and-recap","title":"Quiz and recap","text":"<p>The evaluation of this section will be done with an open-resource quiz covering all cloud computing topics. These concluding slides should be used to recap the previous courses.</p>"},{"location":"1_2_cloud.html","title":"Cloud Computing","text":""},{"location":"1_2_cloud.html#intro-to-cloud-computing","title":"Intro to cloud computing","text":"<p>Link to slides</p> <p> </p>"},{"location":"1_2_cloud.html#using-cloud-computing","title":"Using Cloud Computing","text":"<p>Link to slides</p> <p> </p>"},{"location":"1_2_cloud.html#google-cloud-platform","title":"Google Cloud Platform","text":"<p>Link to slides</p> <p> </p>"},{"location":"1_2_gcp_handson.html","title":"GCP Hands On, first VMs, Google Cloud Storage","text":""},{"location":"1_2_gcp_handson.html#0-abstract","title":"0. Abstract","text":"<p>Abstract</p> <p>In this hands on you will configure your GCP account, the google cloud SDK and access the cloud console using Google Cloud Shell, You will also discover a very useful tool, a managed jupyter notebook service from google named Google Colab which may be very important for your future developments this year</p> <p>Warning</p> <p>It is advised to connect to eduroam if you want to use your local machine</p> <p>Warning</p> <p>Don't forget to shutdown everything after !</p>"},{"location":"1_2_gcp_handson.html#1a-create-your-gcp-account","title":"1a. Create your GCP Account","text":"<p>Note</p> <p>It is possible that you don't have your credits yet, so keep this in mind for when you will be receiving them Skip this for now</p> <p>Overview link</p> <ul> <li>Create an account within Google cloud Platform using your ISAE e-mail</li> <li>Use the code given by Dennis to get your free credits</li> <li>You should have a free tier available to you as well as coupons</li> <li>From the interface you should create a project with a name of your choice</li> </ul>"},{"location":"1_2_gcp_handson.html#1b-selecting-the-general-gcp-project","title":"1b. Selecting the general gcp project","text":"<p>Note</p> <p>If you don't have your project yet you should be added to a global ISAE-SDD project Select it in the interface</p> <p>You should have access to the isae-sdd project and be able to access the interface</p> <p></p>"},{"location":"1_2_gcp_handson.html#2-my-first-vm-github-codespaces","title":"2. My first \"VM\", Github Codespaces","text":""},{"location":"1_2_gcp_handson.html#intro-to-github-codespaces","title":"Intro to Github Codespaces","text":"<ul> <li>Github Codespaces is a \"managed VM\" made available to develop without needing to configure locally your environment.</li> <li>Compared to configured a VM by yourself, this one comes loaded with developer tools, and thus is faster to use,</li> <li>You have a free tier of 60 CPU hours / months and some disk space</li> <li>You pay for the CPI when the VM is ON and for the disk when the codespace is create</li> </ul> <p>Have a look at the overview : https://docs.github.com/en/codespaces/overview </p> <p>Question</p> <ul> <li>Can you describe it with your own words ?</li> </ul> <p>Note</p> <p>Google Cloud has a similar service with Google Cloud Shell but since Codespaces is way more powerful, we will be using that</p>"},{"location":"1_2_gcp_handson.html#create-your-codespace-and-connect-to-it","title":"Create your codespace and connect to it","text":"<p>Go to https://github.com/codespaces</p> <p></p> <ul> <li>Click on the top left corner for a new codespace</li> <li>It should launch a browser with a vscode</li> <li>Launch a terminal using the top right menu</li> </ul> <p>If that does not work, go to https://github.com/github/codespaces-blank and create a codespace from there</p> <p></p>"},{"location":"1_2_gcp_handson.html#explore-github-codespaces","title":"Explore github codespaces","text":"<ul> <li>Check available disk space</li> </ul> Bash command to run <p><code>df -h</code></p> <ul> <li>Check the OS name</li> </ul> Bash command to run <p><code>cat /etc/os-release</code></p> <ul> <li>Check the CPU model</li> </ul> Bash command to run <p><code>cat /proc/cpuinfo</code></p> <ul> <li>This is the hardware model... how many cores do you have available ? Which amount of RAM ?</li> </ul> Help <p><code>htop</code> will give you your current usage and available cores, or you can do <code>nproc</code></p> <ul> <li>Try to upload a file from your computer to the codespace by right clicking on the file explorer on the left</li> </ul>"},{"location":"1_2_gcp_handson.html#a-demo-of-codespace-port-forwarding-web-preview","title":"A demo of codespace port forwarding / web preview","text":"<ul> <li>In your codespace, run <code>jupyter lab</code> to launch the jupyter lab installed in it</li> <li>Check the \"port\" preview : It should have a new entry with the 8888 port. If not, create it manually</li> <li>Click on open in browser</li> <li>Copy the token from your terminal to the web browser</li> <li>You are new in a jupyterlab hosted on your github codespace VM !</li> </ul> <p>Question</p> <p>Magic !? What do you think is happening ? Try to describe it with your own words</p> <ul> <li>Cancel (CTRL+C) the jupyter process</li> </ul> <p>To learn more about port forwarding in codespaces, refer to the documentation</p>"},{"location":"1_2_gcp_handson.html#3-install-google-cloud-sdk-configure-the-shell","title":"3. Install Google Cloud SDK &amp; Configure the shell","text":"<p>If you want to interact with GCP from your computer or codespaces, you will need to install the Google Cloud SDK, which will also install a shell if you are on windows</p> <p>Note</p> <p>You can install the cloud shell locally, but I recommend using your codespace</p> Installing locally <p>The best ways to interact with google cloud SDK is with a terminal so in that order:</p> <ul> <li>Ubuntu / Debian https://cloud.google.com/sdk/docs/install#deb</li> <li>Other Linux (either VM or native): https://cloud.google.com/sdk/docs/install#linux</li> <li>MacOS: https://cloud.google.com/sdk/docs/install#mac</li> <li>Windows Subsystem for Linux: see Linux</li> <li>Windows: https://cloud.google.com/sdk/docs/install#windows</li> </ul> <p>If you are on codespace, follow the following instructions to install the google cloud sdk,</p> <p>Then run <code>gcloud init</code> in your terminal to configure the google cloud sdk with your account</p> <p>You should at some point see at terminal with a link. Click on the link and login with your google accound, then copy the token to your codespace.</p> <p></p> <p>Your github codespace is now configured with your google cloud platform credentials</p>"},{"location":"1_2_gcp_handson.html#4-my-first-google-compute-engine-instance","title":"4. My first Google Compute Engine Instance","text":"<p>First, we will make our first steps by creating a compute engine instance (a vm) using the console, connecting to it via SSH, interacting with it, uploading some files, and we will shut it down and make the magic happen by resizing it</p> <ul> <li>What is google cloud compute engine ? try to describe it with your own words</li> </ul>"},{"location":"1_2_gcp_handson.html#creating-my-vm-using-the-console","title":"Creating my VM using the console","text":"<ul> <li> <p>Create your VM from the google cloud interface : Go to this link and follow the \"CONSOLE\" instruction</p> </li> <li> <p>Create an instance with the following parameters</p> </li> <li>type: n1-standard-1</li> <li>zone: europe-west1-(b,c,d) Belgium</li> <li>os: ubuntu 22.04 x86</li> <li>boot disk size: 10 Gb</li> <li>Give it a name of your choice (that you can remember)</li> <li>DO NOT SHUT IT DOWN for now</li> </ul> Note <p>If you were using the command line, you would have done this</p> <pre><code>gcloud compute instances create {name} --project={your-project} --zone={your-zone} \\\n--machine-type=n1-standard-1 \\\n--image=ubuntu-2204-jammy-v20230114 \\\n--image-project=ubuntu-os-cloud\n</code></pre>"},{"location":"1_2_gcp_handson.html#connecting-to-ssh","title":"Connecting to SSH","text":"<ul> <li>Connect to ssh from the google cloudspace</li> </ul> Solution        `gcloud compute ssh ${MACHINE-NAME}`     <p>Note</p> <p>We are using <code>google compute ssh</code> instead of ssh. This is an automated tool that takes care of locating your machine in GCP and transferring the keys</p> <ul> <li>Check available disk space</li> </ul> Bash command to run      `df -h`     <ul> <li>Check the OS name</li> </ul> Bash command to run      `cat /etc/os-release`     <ul> <li>Check the CPU model</li> </ul> Solution      `cat /proc/cpuinfo`     <ul> <li>Check the number of cores available and the RAM</li> </ul> Solution      `htop`     <ul> <li>Check instance google cloud properties</li> </ul> Solution      `cat /proc/cpuinfo`"},{"location":"1_2_gcp_handson.html#the-magic-of-redimensioning-vms","title":"The magic of redimensioning VMs","text":"<ul> <li>Shutdown the VM (from the web browser), check the previous codelab to see how to do it</li> <li>Select it and click on EDIT</li> <li>Change the machine type to <code>n1-standard-2</code> (link to documentation)</li> <li>Relaunch it, reconnect to it and try to check using <code>htop</code> the number of cores &amp; RAM available</li> <li>Note : If you run <code>cat /proc/cpuinfo</code> again you will see that you are running on the same hardware !</li> </ul> <p>Magic isn't it ? </p> <p>Note: If you had any files and specific configuration, they would still be here !</p>"},{"location":"1_2_gcp_handson.html#optional-but-useful-persistent-ssh-sessions-with-tmux","title":"Optional but useful : Persistent SSH sessions with TMUX","text":"<p>https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/</p> <ul> <li>Connect to your instance using SSH</li> <li>Question: What happens if you start a long computation and disconnect ?</li> <li>Check that tmux is installed on the remote instance (run <code>tmux</code>). if not install it</li> <li>Follow this tutorial: https://www.hamvocke.com/blog/a-quick-and-easy-guide-to-tmux/</li> <li>To check you have understood you should be able to:</li> <li>Connect to your remote instance with ssh</li> <li>Start a tmux session</li> <li>Launch a process (for example <code>htop</code>) inside it</li> <li>Detach from the session (<code>CTRL+B</code> then type <code>:detach</code>)</li> <li>Kill the ssh connection</li> <li>Connect again</li> <li><code>tmux attach</code> to your session</li> <li>Your process should still be here !</li> </ul> <p>Congratulations :)</p>"},{"location":"1_2_gcp_handson.html#transfering-files-from-the-computer-or-codespaces-to-this-machine","title":"Transfering files from the computer (or codespaces) to this machine","text":"<ul> <li>We will use the terminal to transfer some files from* your computer (or codespaces) to** this machine,</li> <li> <p>If you use cloud shell you can do it as well : create a dummy file in cloud shell</p> </li> <li> <p>Follow this link to learn how to use the gcloud cli tool to transfer files to your instance  TOC</p> </li> <li> <p>For experts, it's possible to do it manually using rsync from ssh or scp</p> </li> <li> <p>Transfer some files to your <code>/home/${USER}</code> directory</p> </li> <li> <p>List them from your instance (<code>ls</code>)</p> </li> </ul> <p>How do we do the opposite ?</p> <p>See below,</p>"},{"location":"1_2_gcp_handson.html#5-interacting-with-google-cloud-storage","title":"5. Interacting with Google Cloud Storage","text":"<p>Here we will discover google cloud storage, upload some files from your computer and download them from your instance in the cloud</p> <ul> <li> <p>What is Google Cloud Storage ? Try to describe it with your own words</p> </li> <li> <p>Use this tutorial to upload something from your computer to google cloud storage from the web browser (DO NOT DELETE THE FILES YET)</p> </li> </ul> <p>Now we will download it using the google cloud CLI tool. Here's the documentation</p> <p>Follow the tutorial to learn how to do what you did with <code>gsutil</code></p> <ul> <li>List the content of the bucket you just created (if you deleted it previously, create a new one)</li> <li>Upload a file to a bucket </li> <li>Download a file from a bucket</li> </ul> <p>What if we want to do the same from the VM ?</p> <ul> <li> <p>Now go back to your machine</p> </li> <li> <p>Try to list bucket, download and upload files</p> </li> <li> <p>Is it possible ?</p> </li> <li> <p>If not, it's because you have to allow the instance to access google cloud storage</p> </li> <li> <p>Shutdown the VM and edit it (like we did when we resized the instance)</p> </li> <li> <p>Check \"access scopes\", select \"set access for each api\", and select \"storage / admin\"</p> </li> <li> <p>Now restart you machine, connect back to it. You should be able to upload to google cloud storage now files now</p> </li> <li> <p>Now you can delete the bucket you just created</p> </li> <li> <p>You can delete the VM as well, we will not use it</p> </li> </ul> <p>DELETE THE BUCKET NOW</p>"},{"location":"1_2_gcp_handson.html#6-google-compute-engine-from-the-cli-deep-learning-vms-and-ssh-tunnels","title":"6. Google Compute Engine from the CLI, \"deep learning VMs\" and SSH tunnels","text":"<p>Here we will use the google cloud sdk to create a more complex VM with a pre-installed image and connect to its jupyter server</p> <p>This will be useful for the next part of our workshop because both git and docker are already installed !</p> <p>Google Cloud Platform comes with a set of services targeted at data scientists called AI Platform, among them are Deep Learning VMs which are essentially preinstalled VMs (more or less the same configuration as google colab) with some bonuses.</p> <ul> <li>What are \"Deep Learning VMs\" ? Try to use your own words</li> <li>What would be the alternative if you wanted to get a machine with the same installation ?</li> </ul>"},{"location":"1_2_gcp_handson.html#create-a-google-compute-engine-instance-using-the-command-line","title":"Create a google compute engine instance using the command line","text":"<p>Instead of using the browser to create this machine, we will be using the CLI to create instances</p> <pre><code>export INSTANCE_NAME=\"fch-dlvm-1\" # RENAME THIS !!!!!!!!!!\n\ngcloud compute instances create $INSTANCE_NAME \\\n--zone=\"europe-west1-b\" \\\n--image-family=\"common-cpu\" \\\n--image-project=deeplearning-platform-release \\\n--maintenance-policy=TERMINATE \\\n--scopes=\"storage-rw\" \\\n--machine-type=\"n1-standard-2\" \\\n--boot-disk-size=60GB\n</code></pre> <ul> <li>Notice the similarities between the first VM you created and this one,</li> <li>What changed ?</li> <li>If you want to learn more about compute images, image families etc... go here</li> </ul>"},{"location":"1_2_gcp_handson.html#connect-with-ssh-to-this-machine-and-do-a-port-forwarding","title":"Connect with ssh to this machine and do a port forwarding","text":"<ul> <li> <p>Connect to your instance using the gcloud cli &amp; ssh from the codespace</p> </li> <li> <p>This time, you will forward some ports as well</p> </li> </ul> Solution     gcloud compute ssh user@machine-name --zone=europe-west1-b -- -L 8080:localhost:8080    <p>If you are in codespace, use the port forwarding utility, add a new port (8080). It may be done automatically, you should be in a jupyter notebook under the user <code>jupyter</code></p> <p>Question</p> <p>Where are we ? Where is the jupyter lab hosted ? What is the difference between this and the jupyter lab we launched from codespace</p> <p></p> <p>You can try to play with the jupyter lab (that has a code editor and terminal capabilities) to get a feel of manipulating a remote instance</p> <p>Try to <code>pip3 list</code> to check all dependencies installed !</p> <ul> <li>Delete the instance</li> </ul>"},{"location":"1_2_gcp_handson.html#7-important-cleaning-up","title":"7. IMPORTANT : Cleaning up","text":"<ul> <li>DELETE ALL THE GCP INSTANCES YOU CREATED</li> <li>SHUTDOWN YOUR CODESPACE</li> </ul> <p>Warning</p> <p>Don't forget to delete your instances in GCP</p> <p>How to shutdown codespaces :</p> <p></p> <ul> <li>Click on stop codespace to shut it down (you \"pay\" for the disk with your free credits)</li> <li>Click on kill codespace to delete it</li> </ul>"},{"location":"1_2_gcp_handson.html#8-optional-introduction-to-infrastructure-as-code","title":"8. Optional - Introduction to infrastructure as code","text":"<ul> <li> <p>This tutorial will guide you through google cloud deployment manager, which is a way to deploy google compute engine instances using configuration files</p> </li> <li> <p>Don't forget to adapt machine configurations and zone to your use case (see above)</p> </li> </ul> <p>If you run this, don't forget to clean everything up afterwards</p>"},{"location":"1_2_gcp_handson.html#9-optional-google-colaboratory","title":"9. Optional - Google Colaboratory","text":"<p>Abstract</p> <p>Previous versions of this class happened before the ML Class so there was an introduction to google collab. You should have extensively used this tool before, so skip this :)</p> <p>Here, you will look at Google Colaboratory, which is a very handy tool for doing data science work (based on jupyter notebooks) on the cloud, using a preconfigured instance (which can access a GPU). You will be able to store data on Google Drive and to share</p> <p>I highly recommend using this for Jupyter based AML BE, but I invite you to discover google colab at home, or during AML BE because it's a useful tool but mastering it is not relevant for our cloud class</p>"},{"location":"1_2_gcp_handson.html#intro-description-of-google-colaboratory","title":"Intro &amp; Description of Google Colaboratory","text":"<ul> <li>Open Google Colab</li> <li>Some intro, another one</li> </ul> <p>Question</p> <ul> <li>Can you describe what it is ?</li> <li>Is it IaaS ? PaaS ? SaaS ? why exactly ?</li> </ul> <p>Info</p> <p>Colaboratory, or \"Colab\" for short, allows you to write and execute Python in your browser, with</p> <ul> <li>Zero configuration required</li> <li>Free access to GPUs</li> <li>Easy sharing</li> </ul> <p>It offers a \"jupyter notebook - like\" interface, and allows to install your own dependencies by running bash commands inside the VM, with connection to google drive, google sheets</p> <p>You can manipulate the notebooks from your Google Drive and share it like it was a GDoc document</p> <p>It's essentially between SaaS and PaaS, it offers you a development platform without you having to manage anything except your code and your data (which are both data from the cloud provider point of view)</p>"},{"location":"1_2_gcp_handson.html#loading-jupyter-notebooks-interacting-with-google-drive","title":"Loading jupyter notebooks, interacting with google drive","text":"<ul> <li>Open a notebook you previously ran on your computer (from AML class), you can run a notebook on github directly in google colab</li> <li>Try to run it inside google colab</li> <li>Link google colab and google drive and upload something on google drive (like an image) and display in on google colab</li> </ul>"},{"location":"1_2_gcp_handson.html#other-nice-usages-of-google-colab","title":"Other nice usages of Google Colab","text":"<ul> <li>Writing markdown to generate reports</li> <li>Installing custom dependencies</li> </ul>"},{"location":"1_3_containers.html","title":"From Virtualisation to Containerisation","text":"<p>Link to slides</p> <p> </p>"},{"location":"1_3_docker_tp.html","title":"Docker: Hands on","text":""},{"location":"1_3_docker_tp.html#0-how-to-run-this","title":"0. How to run this ?","text":"<p>Abstract</p> <p>We will discover the basics of docker and you will be able to manipulate your first images and containers !</p> <p>You should be inside the Github Codespace you created and have google cloud SDK installed in it</p> <p>If not, refer to the previous tutorial and do step 2 and 3</p> <p>This codespace has everything you need, including docker</p> <p>If you want to do everything from your linux machine you can install docker but I don't recommend it for now</p>"},{"location":"1_3_docker_tp.html#1-manipulating-docker-for-the-1st-time","title":"1. Manipulating docker for the 1st time","text":"<p>Source: https://github.com/docker/labs</p> <p>To get started, let's run the following in our terminal:</p> <pre><code>$ docker pull alpine\n</code></pre> <p>The <code>pull</code> command fetches the alpine image from the Docker registry and saves it in our system. You can use the <code>docker images</code> command to see a list of all images on your system. <pre><code>$ docker images\nREPOSITORY              TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nalpine                 latest              c51f86c28340        4 weeks ago         1.109 MB\nhello-world             latest              690ed74de00f        5 months ago        960 B\n</code></pre></p>"},{"location":"1_3_docker_tp.html#11-docker-run","title":"1.1 Docker Run","text":"<p>Great! Let's now run a Docker container based on this image. To do that you are going to use the <code>docker run</code> command.</p> <pre><code>$ docker run alpine ls -l\ntotal 48\ndrwxr-xr-x    2 root     root          4096 Mar  2 16:20 bin\ndrwxr-xr-x    5 root     root           360 Mar 18 09:47 dev\ndrwxr-xr-x   13 root     root          4096 Mar 18 09:47 etc\ndrwxr-xr-x    2 root     root          4096 Mar  2 16:20 home\ndrwxr-xr-x    5 root     root          4096 Mar  2 16:20 lib\n......\n......\n</code></pre> <p>What happened? Behind the scenes, a lot of stuff happened. When you call <code>run</code>,</p> <ol> <li>The Docker client contacts the Docker daemon</li> <li>The Docker daemon checks local store if the image (alpine in this case) is available locally, and if not, downloads it from Docker Store. (Since we have issued <code>docker pull alpine</code> before, the download step is not necessary)</li> <li>The Docker daemon creates the container and then runs a command in that container.</li> <li>The Docker daemon streams the output of the command to the Docker client</li> </ol> <p>When you run <code>docker run alpine</code>, you provided a command (<code>ls -l</code>), so Docker started the command specified and you saw the listing.</p> <p>Let's try something more exciting.</p> <p><pre><code>$ docker run alpine echo \"hello from alpine\"\nhello from alpine\n</code></pre> OK, that's some actual output. In this case, the Docker client dutifully ran the <code>echo</code> command in our alpine container and then exited it. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast!</p> <p>Try another command.</p> <pre><code>docker run alpine /bin/sh\n</code></pre> <p>Wait, nothing happened! Is that a bug? Well, no. These interactive shells will exit after running any scripted commands, unless they are run in an interactive terminal - so for this example to not exit, you need to <code>docker run -it alpine /bin/sh</code>.</p> <p>You are now inside the container shell and you can try out a few commands like <code>ls -l</code>, <code>uname -a</code> and others. Exit out of the container by giving the <code>exit</code> command.</p> <p>Ok, now it's time to see the <code>docker ps</code> command. The <code>docker ps</code> command shows you all containers that are currently running.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n</code></pre> <p>Since no containers are running, you see a blank line. Let's try a more useful variant: <code>docker ps -a</code></p> <pre><code>$ docker ps -a\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                      PORTS               NAMES\n36171a5da744        alpine              \"/bin/sh\"                5 minutes ago       Exited (0) 2 minutes ago                        fervent_newton\na6a9d46d0b2f        alpine             \"echo 'hello from alp\"    6 minutes ago       Exited (0) 6 minutes ago                        lonely_kilby\nff0a5c3750b9        alpine             \"ls -l\"                   8 minutes ago       Exited (0) 8 minutes ago                        elated_ramanujan\nc317d0a9e3d2        hello-world         \"/hello\"                 34 seconds ago      Exited (0) 12 minutes ago                       stupefied_mcclintock\n</code></pre> <p>What you see above is a list of all containers that you ran. Notice that the <code>STATUS</code> column shows that these containers exited a few minutes ago. You're probably wondering if there is a way to run more than just one command in a container. Let's try that now:</p> <p><pre><code>$ docker run -it alpine /bin/sh\n/ # ls\nbin      dev      etc      home     lib      linuxrc  media    mnt      proc     root     run      sbin     sys      tmp      usr      var\n/ # uname -a\nLinux 97916e8cb5dc 4.4.27-moby #1 SMP Wed Oct 26 14:01:48 UTC 2016 x86_64 Linux\n</code></pre> Running the <code>run</code> command with the <code>-it</code> flags attaches us to an interactive tty in the container. Now you can run as many commands in the container as you want. Take some time to run your favorite commands.</p> <p>That concludes a whirlwind tour of the <code>docker run</code> command which would most likely be the command you'll use most often. It makes sense to spend some time getting comfortable with it. To find out more about <code>run</code>, use <code>docker run --help</code> to see a list of all flags it supports. As you proceed further, we'll see a few more variants of <code>docker run</code>.</p>"},{"location":"1_3_docker_tp.html#12-terminology","title":"1.2 Terminology","text":"<p>In the last section, you saw a lot of Docker-specific jargon which might be confusing to some. So before you go further, let's clarify some terminology that is used frequently in the Docker ecosystem.</p> <ul> <li>Images - The file system and configuration of our application which are used to create containers. To find out more about a Docker image, run <code>docker inspect alpine</code>. In the demo above, you used the <code>docker pull</code> command to download the alpine image. When you executed the command <code>docker run hello-world</code>, it also did a <code>docker pull</code> behind the scenes to download the hello-world image.</li> <li>Containers - Running instances of Docker images \u2014 containers run the actual applications. A container includes an application and all of its dependencies. It shares the kernel with other containers, and runs as an isolated process in user space on the host OS. You created a container using <code>docker run</code> which you did using the alpine image that you downloaded. A list of running containers can be seen using the <code>docker ps</code> command.</li> <li>Docker daemon - The background service running on the host that manages building, running and distributing Docker containers.</li> <li>Docker client - The command line tool that allows the user to interact with the Docker daemon.</li> <li>Docker Store - A registry of Docker images, where you can find trusted and enterprise ready containers, plugins, and Docker editions. You'll be using this later in this tutorial.</li> </ul>"},{"location":"1_3_docker_tp.html#20-webapps-with-docker","title":"2.0 Webapps with Docker","text":"<p>Source: https://github.com/docker/labs</p> <p>Great! So you have now looked at <code>docker run</code>, played with a Docker container and also got the hang of some terminology. Armed with all this knowledge, you are now ready to get to the real stuff \u2014 deploying web applications with Docker.</p>"},{"location":"1_3_docker_tp.html#21-run-a-static-website-in-a-container","title":"2.1 Run a static website in a container","text":"<p>Note: Code for this section is in this repo in the website directory</p> <p>Let's start by taking baby-steps. First, we'll use Docker to run a static website in a container. The website is based on an existing image. We'll pull a Docker image from Docker Store, run the container, and see how easy it is to set up a web server.</p> <p>The image that you are going to use is a single-page website that was already created for this demo and is available on the Docker Store as <code>dockersamples/static-site</code>. You can download and run the image directly in one go using <code>docker run</code> as follows.</p> <pre><code>docker run -d dockersamples/static-site\n</code></pre> <p>Files:</p> <ul> <li>Dockerfile</li> <li>hello_docker.html</li> </ul> <p>Note: The current version of this image doesn't run without the <code>-d</code> flag. The <code>-d</code> flag enables detached mode, which detaches the running container from the terminal/shell and returns your prompt after the container starts. We are debugging the problem with this image but for now, use <code>-d</code> even for this first example.</p> <p>So, what happens when you run this command?</p> <p>Since the image doesn't exist on your Docker host, the Docker daemon first fetches it from the registry and then runs it as a container.</p> <p>Now that the server is running, do you see the website? What port is it running on? And more importantly, how do you access the container directly from our host machine?</p> <p>Actually, you probably won't be able to answer any of these questions yet! \u263a In this case, the client didn't tell the Docker Engine to publish any of the ports, so you need to re-run the <code>docker run</code> command to add this instruction.</p> <p>Let's re-run the command with some new flags to publish ports and pass your name to the container to customize the message displayed. We'll use the -d option again to run the container in detached mode.</p> <p>First, stop the container that you have just launched. In order to do this, we need the container ID.</p> <p>Since we ran the container in detached mode, we don't have to launch another terminal to do this. Run <code>docker ps</code> to view the running containers.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS               NAMES\na7a0e504ca3e        dockersamples/static-site   \"/bin/sh -c 'cd /usr/\"   28 seconds ago      Up 26 seconds       80/tcp, 443/tcp     stupefied_mahavira\n</code></pre> <p>Check out the <code>CONTAINER ID</code> column. You will need to use this <code>CONTAINER ID</code> value, a long sequence of characters, to identify the container you want to stop, and then to remove it. The example below provides the <code>CONTAINER ID</code> on our system; you should use the value that you see in your terminal.</p> <pre><code>$ docker stop a7a0e504ca3e\n$ docker rm   a7a0e504ca3e\n</code></pre> <p>Note: A cool feature is that you do not need to specify the entire <code>CONTAINER ID</code>. You can just specify a few starting characters and if it is unique among all the containers that you have launched, the Docker client will intelligently pick it up.</p> <p>Now, let's launch a container in detached mode as shown below:</p> <pre><code>$ docker run --name static-site -e AUTHOR=\"Your Name\" -d -P dockersamples/static-site\ne61d12292d69556eabe2a44c16cbd54486b2527e2ce4f95438e504afb7b02810\n</code></pre> <p>In the above command:</p> <ul> <li><code>-d</code> will create a container with the process detached from our terminal</li> <li><code>-P</code> will publish all the exposed container ports to random ports on the Docker host</li> <li><code>-e</code> is how you pass environment variables to the container</li> <li><code>--name</code> allows you to specify a container name</li> <li><code>AUTHOR</code> is the environment variable name and <code>Your Name</code> is the value that you can pass</li> </ul> <p>Now you can see the ports by running the <code>docker port</code> command.</p> <pre><code>$ docker port static-site\n443/tcp -&gt; 0.0.0.0:32772\n80/tcp -&gt; 0.0.0.0:32773\n</code></pre> <p>If you are on codespace, create a port forwarding on port 80 to connect to the website</p> <p>If you are running Docker for Mac, Docker for Windows, or Docker on Linux, you can open <code>http://localhost:[YOUR_PORT_FOR 80/tcp]</code>. For our example this is <code>http://localhost:32773</code>.</p> <p>If you are using Docker Machine on Mac or Windows, you can find the hostname on the command line using <code>docker-machine</code> as follows (assuming you are using the <code>default</code> machine).</p> <p><pre><code>$ docker-machine ip default\n192.168.99.100\n</code></pre> You can now open <code>http://&lt;YOUR_IPADDRESS&gt;:[YOUR_PORT_FOR 80/tcp]</code> to see your site live! For our example, this is: <code>http://192.168.99.100:32773</code>.</p> <p>You can also run a second webserver at the same time, specifying a custom host port mapping to the container's webserver.</p> <pre><code>$ docker run --name static-site-2 -e AUTHOR=\"Your Name\" -d -p 8888:80 dockersamples/static-site\n</code></pre> <p></p> <p>To deploy this on a real server you would just need to install Docker, and run the above <code>docker</code> command(as in this case you can see the <code>AUTHOR</code> is Docker which we passed as an environment variable).</p> <p>Now that you've seen how to run a webserver inside a Docker container, how do you create your own Docker image? This is the question we'll explore in the next section.</p> <p>But first, let's stop and remove the containers since you won't be using them anymore.</p> <pre><code>$ docker stop static-site\n$ docker rm static-site\n</code></pre> <p>Let's use a shortcut to remove the second site:</p> <pre><code>$ docker rm -f static-site-2\n</code></pre> <p>Run <code>docker ps</code> to make sure the containers are gone.</p> <pre><code>$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n</code></pre>"},{"location":"1_3_docker_tp.html#22-docker-images","title":"2.2 Docker Images","text":"<p>In this section, let's dive deeper into what Docker images are. You will build your own image, use that image to run an application locally.</p> <p>Docker images are the basis of containers. In the previous example, you pulled the dockersamples/static-site image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally on your system, run the <code>docker images</code> command.</p> <pre><code>$ docker images\nREPOSITORY             TAG                 IMAGE ID            CREATED             SIZE\ndockersamples/static-site   latest              92a386b6e686        2 hours ago        190.5 MB\nnginx                  latest              af4b3d7d5401        3 hours ago        190.5 MB\npython                 2.7                 1c32174fd534        14 hours ago        676.8 MB\npostgres               9.4                 88d845ac7a88        14 hours ago        263.6 MB\ncontainous/traefik     latest              27b4e0c6b2fd        4 days ago          20.75 MB\nnode                   0.10                42426a5cba5f        6 days ago          633.7 MB\nredis                  latest              4f5f397d4b7c        7 days ago          177.5 MB\nmongo                  latest              467eb21035a8        7 days ago          309.7 MB\nalpine                 3.3                 70c557e50ed6        8 days ago          4.794 MB\njava                   7                   21f6ce84e43c        8 days ago          587.7 MB\n</code></pre> <p>Above is a list of images that I've pulled from the registry and those I've created myself (we'll shortly see how). You will have a different list of images on your machine. The <code>TAG</code> refers to a particular snapshot of the image and the <code>ID</code> is the corresponding unique identifier for that image.</p> <p>For simplicity, you can think of an image akin to a git repository - images can be committed with changes and have multiple versions. When you do not provide a specific version number, the client defaults to <code>latest</code>.</p> <p>For example you could pull a specific version of <code>ubuntu</code> image as follows:</p> <pre><code>$ docker pull ubuntu:12.04\n</code></pre> <p>If you do not specify the version number of the image then, as mentioned, the Docker client will default to a version named <code>latest</code>.</p> <p>So for example, the <code>docker pull</code> command given below will pull an image named <code>ubuntu:latest</code>:</p> <pre><code>$ docker pull ubuntu\n</code></pre> <p>To get a new Docker image you can either get it from a registry (such as the Docker Store) or create your own. There are hundreds of thousands of images available on Docker Store. You can also search for images directly from the command line using <code>docker search</code>.</p> <p>An important distinction with regard to images is between base images and child images.</p> <ul> <li> <p>Base images are images that have no parent images, usually images with an OS like ubuntu, alpine or debian.</p> </li> <li> <p>Child images are images that build on base images and add additional functionality.</p> </li> </ul> <p>Another key concept is the idea of official images and user images. (Both of which can be base images or child images.)</p> <ul> <li> <p>Official images are Docker sanctioned images. Docker, Inc. sponsors a dedicated team that is responsible for reviewing and publishing all Official Repositories content. This team works in collaboration with upstream software maintainers, security experts, and the broader Docker community. These are not prefixed by an organization or user name. In the list of images above, the <code>python</code>, <code>node</code>, <code>alpine</code> and <code>nginx</code> images are official (base) images. To find out more about them, check out the Official Images Documentation.</p> </li> <li> <p>User images are images created and shared by users like you. They build on base images and add additional functionality. Typically these are formatted as <code>user/image-name</code>. The <code>user</code> value in the image name is your Docker Store user or organization name.</p> </li> </ul>"},{"location":"1_3_docker_tp.html#23-create-your-first-image","title":"2.3 Create your first image","text":"<p>Note: The code for this section is in this repository in the flask-app directory.</p> <p>Now that you have a better understanding of images, it's time to create your own. Our goal here is to create an image that sandboxes a small Flask application.</p> <p>The goal of this exercise is to create a Docker image which will run a Flask app.</p> <p>We'll do this by first pulling together the components for a random cat picture generator built with Python Flask, then dockerizing it by writing a Dockerfile. Finally, we'll build the image, and then run it.</p> <ul> <li>Create a Python Flask app that displays random cat pix</li> <li>Write a Dockerfile</li> <li>Build the image</li> <li>Run your image</li> <li>Dockerfile commands summary</li> </ul>"},{"location":"1_3_docker_tp.html#231-create-a-python-flask-app-that-displays-random-cat-pix","title":"2.3.1 Create a Python Flask app that displays random cat pix","text":"<p>For the purposes of this workshop, we've created a fun little Python Flask app that displays a random cat <code>.gif</code> every time it is loaded - because, you know, who doesn't like cats?</p> <p>Start by creating a directory called <code>flask-app</code> where we'll create the following files:</p> <ul> <li>app.py</li> <li>requirements.txt</li> <li>templates/index.html</li> <li>Dockerfile</li> </ul> <p>Make sure to <code>cd flask-app</code> before you start creating the files, because you don't want to start adding a whole bunch of other random files to your image.</p>"},{"location":"1_3_docker_tp.html#apppy","title":"app.py","text":"<p>Create the app.py with the following content:</p> <pre><code>from flask import Flask, render_template\nimport random\n\napp = Flask(__name__)\n\n# list of cat images\nimages = [\n   \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif1.gif\",\n   \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif2.gif\",\n   \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif3.gif\",\n   \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif4.gif\",\n   \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif5.gif\",\n   \"https://storage.googleapis.com/fchouteau-isae-cloud/gifs/gif6.gif\",\n    ]\n\n@app.route('/')\ndef index():\n    url = random.choice(images)\n    return render_template('index.html', url=url)\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\")\n</code></pre>"},{"location":"1_3_docker_tp.html#requirementstxt","title":"requirements.txt","text":"<p>In order to install the Python modules required for our app, we need to create a file called requirements.txt and add the following line to that file:</p> <pre><code>flask\ntyper\n</code></pre>"},{"location":"1_3_docker_tp.html#templatesindexhtml","title":"templates/index.html","text":"<p>Create a directory called <code>templates</code> and create an index.html file in that directory with the following content in it:</p> <pre><code>&lt;html&gt;\n  &lt;head&gt;\n    &lt;style type=\"text/css\"&gt;\nbody {\nbackground: black;\ncolor: white;\n}\ndiv.container {\nmax-width: 500px;\nmargin: 100px auto;\nborder: 20px solid white;\npadding: 10px;\ntext-align: center;\n}\nh4 {\ntext-transform: uppercase;\n}\n&lt;/style&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div class=\"container\"&gt;\n      &lt;h4&gt;Cat Gif of the day&lt;/h4&gt;\n      &lt;img src=\"{{url}}\" /&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"1_3_docker_tp.html#232-write-a-dockerfile","title":"2.3.2 Write a Dockerfile","text":"<p>We want to create a Docker image with this web app. As mentioned above, all user images are based on a base image. Since our application is written in Python, we will build our own Python image based on Alpine. We'll do that using a Dockerfile.</p> <p>A Dockerfile is a text file that contains a list of commands that the Docker daemon calls while creating an image. The Dockerfile contains all the information that Docker needs to know to run the app \u2014 a base Docker image to run from, location of your project code, any dependencies it has, and what commands to run at start-up. It is a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own Dockerfiles.</p> <ol> <li>Create a file called Dockerfile, and add content to it as described below.</li> </ol> <p>We'll start by specifying our base image, using the <code>FROM</code> keyword:</p> <pre><code>FROM alpine:3.15\n</code></pre> <ol> <li>The next step usually is to write the commands of copying the files and installing the dependencies. But first we will install the Python pip package to the alpine linux distribution. This will not just install the pip package but any other dependencies too, which includes the python interpreter. Add the following RUN command next:</li> </ol> <pre><code>RUN apk add --update py3-pip\n</code></pre> <ol> <li>Let's add the files that make up the Flask Application.</li> </ol> <p>Install all Python requirements for our app to run. This will be accomplished by adding the lines:</p> <pre><code>COPY requirements.txt /usr/src/app/\nRUN pip install --no-cache-dir -r /usr/src/app/requirements.txt\n</code></pre> <p>Copy the files you have created earlier into our image by using COPY  command.</p> <pre><code>COPY app.py /usr/src/app/\nCOPY templates/index.html /usr/src/app/templates/\n</code></pre> <ol> <li>Specify the port number which needs to be exposed. Since our flask app is running on <code>5000</code> that's what we'll expose.</li> </ol> <pre><code>EXPOSE 5000\n</code></pre> <ol> <li>The last step is the command for running the application which is simply - <code>python ./app.py</code>. Use the CMD command to do that:</li> </ol> <pre><code>CMD [\"python3\", \"/usr/src/app/app.py\"]\n</code></pre> <p>The primary purpose of <code>CMD</code> is to tell the container which command it should run by default when it is started.</p> <ol> <li>Verify your Dockerfile.</li> </ol> <p>Our <code>Dockerfile</code> is now ready. This is how it looks:</p> <pre><code># our base image\nFROM alpine:3.15\n\n# Install python and pip\nRUN apk add --update py3-pip\n\n# install Python modules needed by the Python app\nCOPY requirements.txt /usr/src/app/\nRUN pip3 install --no-cache-dir -r /usr/src/app/requirements.txt\n\n# copy files required for the app to run\nCOPY app.py /usr/src/app/\nCOPY templates/index.html /usr/src/app/templates/\n\n# tell the port number the container should expose\nEXPOSE 5000\n\n# run the application\nCMD [\"python3\", \"/usr/src/app/app.py\"]\n</code></pre>"},{"location":"1_3_docker_tp.html#233-build-the-image","title":"2.3.3 Build the image","text":"<p>Now that you have your <code>Dockerfile</code>, you can build your image. The <code>docker build</code> command does the heavy-lifting of creating a docker image from a <code>Dockerfile</code>.</p> <p>The <code>docker build</code> command is quite simple - it takes an optional tag name with the <code>-t</code> flag, and the location of the directory containing the <code>Dockerfile</code> - the <code>.</code> indicates the current directory:</p> <p><code>docker build -t myfirstapp:1.0 .</code></p> <pre><code>$ docker build -t myfirstapp:1.0 .\nSending build context to Docker daemon 9.728 kB\nStep 1 : FROM alpine:latest\n ---&gt; 0d81fc72e790\nStep 2 : RUN apk add --update py-pip\n ---&gt; Running in 8abd4091b5f5\nfetch http://dl-4.alpinelinux.org/alpine/v3.3/main/x86_64/APKINDEX.tar.gz\nfetch http://dl-4.alpinelinux.org/alpine/v3.3/community/x86_64/APKINDEX.tar.gz\n(1/12) Installing libbz2 (1.0.6-r4)\n(2/12) Installing expat (2.1.0-r2)\n(3/12) Installing libffi (3.2.1-r2)\n(4/12) Installing gdbm (1.11-r1)\n(5/12) Installing ncurses-terminfo-base (6.0-r6)\n(6/12) Installing ncurses-terminfo (6.0-r6)\n(7/12) Installing ncurses-libs (6.0-r6)\n(8/12) Installing readline (6.3.008-r4)\n(9/12) Installing sqlite-libs (3.9.2-r0)\n(10/12) Installing python (2.7.11-r3)\n(11/12) Installing py-setuptools (18.8-r0)\n(12/12) Installing py-pip (7.1.2-r0)\nExecuting busybox-1.24.1-r7.trigger\nOK: 59 MiB in 23 packages\n ---&gt; 976a232ac4ad\nRemoving intermediate container 8abd4091b5f5\nStep 3 : COPY requirements.txt /usr/src/app/\n ---&gt; 65b4be05340c\nRemoving intermediate container 29ef53b58e0f\nStep 4 : RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt\n ---&gt; Running in a1f26ded28e7\nCollecting Flask==0.10.1 (from -r /usr/src/app/requirements.txt (line 1))\n  Downloading Flask-0.10.1.tar.gz (544kB)\nCollecting Werkzeug&gt;=0.7 (from Flask==0.10.1-&gt;-r /usr/src/app/requirements.txt (line 1))\n  Downloading Werkzeug-0.11.4-py2.py3-none-any.whl (305kB)\nCollecting Jinja2&gt;=2.4 (from Flask==0.10.1-&gt;-r /usr/src/app/requirements.txt (line 1))\n  Downloading Jinja2-2.8-py2.py3-none-any.whl (263kB)\nCollecting itsdangerous&gt;=0.21 (from Flask==0.10.1-&gt;-r /usr/src/app/requirements.txt (line 1))\n  Downloading itsdangerous-0.24.tar.gz (46kB)\nCollecting MarkupSafe (from Jinja2&gt;=2.4-&gt;Flask==0.10.1-&gt;-r /usr/src/app/requirements.txt (line 1))\n  Downloading MarkupSafe-0.23.tar.gz\nInstalling collected packages: Werkzeug, MarkupSafe, Jinja2, itsdangerous, Flask\n  Running setup.py install for MarkupSafe\n  Running setup.py install for itsdangerous\n  Running setup.py install for Flask\nSuccessfully installed Flask-0.10.1 Jinja2-2.8 MarkupSafe-0.23 Werkzeug-0.11.4 itsdangerous-0.24\nYou are using pip version 7.1.2, however version 8.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\n ---&gt; 8de73b0730c2\nRemoving intermediate container a1f26ded28e7\nStep 5 : COPY app.py /usr/src/app/\n ---&gt; 6a3436fca83e\nRemoving intermediate container d51b81a8b698\nStep 6 : COPY templates/index.html /usr/src/app/templates/\n ---&gt; 8098386bee99\nRemoving intermediate container b783d7646f83\nStep 7 : EXPOSE 5000\n ---&gt; Running in 31401b7dea40\n ---&gt; 5e9988d87da7\nRemoving intermediate container 31401b7dea40\nStep 8 : CMD python /usr/src/app/app.py\n ---&gt; Running in 78e324d26576\n ---&gt; 2f7357a0805d\nRemoving intermediate container 78e324d26576\nSuccessfully built 2f7357a0805d\n</code></pre> <p>If you don't have the <code>alpine:3.5</code> image, the client will first pull the image and then create your image. Therefore, your output on running the command will look different from mine. If everything went well, your image should be ready! Run <code>docker images</code> and see if your image (<code>&lt;YOUR_USERNAME&gt;/myfirstapp</code>) shows.</p>"},{"location":"1_3_docker_tp.html#234-run-your-image","title":"2.3.4 Run your image","text":"<p>The next step in this section is to run the image and see if it actually works.</p> <pre><code>$ docker run -p 8888:5000 --name myfirstapp myfirstapp:1.0\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n</code></pre> <p>Head over to <code>http://localhost:8888</code> and your app should be live. Note If you are using Docker Machine, you may need to open up another terminal and determine the container ip address using <code>docker-machine ip default</code>.</p> <p></p> <p>Hit the Refresh button in the web browser to see a few more cat images.</p>"},{"location":"1_3_docker_tp.html#235-dockerfile-commands-summary","title":"2.3.5 Dockerfile commands summary","text":"<p>Here's a quick summary of the few basic commands we used in our Dockerfile.</p> <ul> <li> <p><code>FROM</code> starts the Dockerfile. It is a requirement that the Dockerfile must start with the <code>FROM</code> command. Images are created in layers, which means you can use another image as the base image for your own. The <code>FROM</code> command defines your base layer. As arguments, it takes the name of the image. Optionally, you can add the Docker Cloud username of the maintainer and image version, in the format <code>username/imagename:version</code>.</p> </li> <li> <p><code>RUN</code> is used to build up the Image you're creating. For each <code>RUN</code> command, Docker will run the command then create a new layer of the image. This way you can roll back your image to previous states easily. The syntax for a <code>RUN</code> instruction is to place the full text of the shell command after the <code>RUN</code> (e.g., <code>RUN mkdir /user/local/foo</code>). This will automatically run in a <code>/bin/sh</code> shell. You can define a different shell like this: <code>RUN /bin/bash -c 'mkdir /user/local/foo'</code></p> </li> <li> <p><code>COPY</code> copies local files into the container.</p> </li> <li> <p><code>CMD</code> defines the commands that will run on the Image at start-up. Unlike a <code>RUN</code>, this does not create a new layer for the Image, but simply runs the command. There can only be one <code>CMD</code> per a Dockerfile/Image. If you need to run multiple commands, the best way to do that is to have the <code>CMD</code> run a script. <code>CMD</code> requires that you tell it where to run the command, unlike <code>RUN</code>. So example <code>CMD</code> commands would be:</p> </li> </ul> <pre><code>  CMD [\"python\", \"./app.py\"]\n\n  CMD [\"/bin/bash\", \"echo\", \"Hello World\"]\n</code></pre> <ul> <li><code>EXPOSE</code> creates a hint for users of an image which ports provide services. It is included in the information which  can be retrieved via <code>$ docker inspect &lt;container-id&gt;</code>.     </li> </ul> <p>Note: The <code>EXPOSE</code> command does not actually make any ports accessible to the host! Instead, this requires  publishing ports by means of the <code>-p</code> flag when using <code>$ docker run</code>.  </p> <ul> <li><code>PUSH</code> pushes your image to Docker Cloud, or alternately to a private registry</li> </ul> <p>Note: If you want to learn more about Dockerfiles, check out Best practices for writing Dockerfiles.</p>"},{"location":"1_3_docker_tp.html#3-running-cli-apps-packaged-in-docker-while-mounting-volumes","title":"3. Running CLI apps packaged in docker while mounting volumes","text":"<p>Instead of serving web app you can also run applications, like command line interfaces or training scripts, packaged in docker. It is very useful to deliver packaged apps with specific installation to other users. This is often used when you want to package your machine learning environment to run training in distributed fashion.</p> <p>Usually you just edit config files in an external editor and pass it to the docker</p> <ul> <li>Let's modify the <code>app.py</code> in 2. with the following</li> </ul> <pre><code>import typer\nfrom typing import Optional\nfrom pathlib import Path\n\napp = typer.Typer()\n\n\n@app.command()\ndef hello(name: str):\n    typer.echo(f\"Hello {name}\")\n\n@app.command()\ndef run_config(config: Optional[Path] = typer.Option(None)):\n    if config is None:\n        typer.echo(\"No config file\")\n        raise typer.Abort()\n    if config.is_file():\n        text = config.read_text()\n        typer.echo(f\"Config file contents:\\n{text}\")\n    elif config.is_dir():\n        typer.echo(\"Config is a directory, will use all its config files\")\n    elif not config.exists():\n        typer.echo(\"The config doesn't exist\")\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>With your terminal you would call it using <code>python app.py hello {my name}</code> or <code>python app.py run-config --config-file {my config}</code></p> <ul> <li>Modify the dockerfile : </li> <li>Replace <code>CMD [\"python3\", \"/usr/src/app/app.py\"]</code></li> <li> <p>By <code>ENTRYPOINT [\"python3\", \"/usr/src/app/app.py\"]</code></p> </li> <li> <p>Rebuild your docker image (maybe git it another name)</p> </li> <li> <p>Now to run the CLI you just have to <code>docker run --rm {your image} {your args}</code>. Try it with <code>docker run {...} hello {your name}</code></p> </li> </ul> <p>In order to pass a config file, or data to your docker, you need to make it available to your docker. To do that, we have to mount volumes</p> <p>Create a dummy config file (<code>config.txt</code>) in another folder (ex: <code>config/</code>) then mount it when you run the docker:</p> <pre><code>docker run --rm \\\n  -v /home/${USER}/configs:/home/configs \\\n  --workdir /home/ \\\n  {your image} \\\n  run-config --config-file {path to your config in DOCKER, eg /home/configs/config.txt}\n</code></pre> <p>Note that since you mounted volumes, you must pass the path in the docker to your config file for it to work</p>"},{"location":"1_3_docker_tp.html#4-containers-registry","title":"4. Containers Registry","text":"<p>Remember Container Registries ? Here as some explainers</p> <p>The main container registry is dockerhub, https://hub.docker.com/</p> <p>All docker engines that have access to the internet have access to this main hub, and this is where we pulled our base images from before</p> <p>Example, the Python Image</p> <p>Google Cloud has a Container Registry per project, which ensures the docker images you build are accessible for the people who have access to your project only.</p> <p>However, it requires naming the image in a specific fashion: <code>eu.gcr.io/${PROJECT_ID}/name:tag</code></p> <ul> <li>Use the docker cli to tag your previous myfirstapp image to the right namespace</li> </ul> <p><code>docker tag myfirstapp eu.gcr.io/{PROJECT_ID}/{a-unique-name-describing-your-app}:1.0</code></p> <ul> <li> <p>Upload it on container registry <code>docker push [HOSTNAME]/[PROJECT-ID]/[IMAGE]:[TAG]</code></p> </li> <li> <p>If you have a problem of authentification, <code>gcloud auth configure-docker</code></p> </li> </ul> <p>Hint</p> <p>to get your project id: <code>PROJECT_ID=$(gcloud config get-value project 2&gt; /dev/null)</code></p> <ul> <li>Go to container registry https://console.cloud.google.com/gcr, you should see your docker image :)</li> </ul>"},{"location":"1_3_docker_tp.html#5-bonus-data-science-standardized-environment-and-mounting-volumes","title":"5. Bonus - Data Science Standardized Environment and mounting volumes","text":"<p>Note : This may not run in your native github codespace due to the storage available. If you encounter a storage error, run <code>docker system prune</code> to cleanup everything</p>"},{"location":"1_3_docker_tp.html#51-intro","title":"5.1 Intro","text":"<p>Those of us who work on a team know how hard it is to create a standardize development environment. Or if you have ever updated a dependency and had everything break, you understand the importance of keeping development environments isolated.</p> <p>Using Docker, we can create a project / team image with our development environment and mount a volume with our notebooks and data.</p> <p>The benefits of this workflow are that we can:</p> <ul> <li>Separate out projects</li> <li>Spin up a container to onboard new employees</li> <li>Build an automated testing pipeline to confirm upgrade dependencies do not break code</li> </ul>"},{"location":"1_3_docker_tp.html#52-jupyter-stack-docker-image","title":"5.2 Jupyter Stack Docker Image","text":"<p>For this exercise we will use Jupyter Stack Docker Image which is a fully configured docker image that can be used as a data science container</p> <p>Take a look at the documentation and the dockerhub repository</p> <p>To get the docker image, run</p> <pre><code>docker pull jupyter/scipy-notebook:lab-3.5.3\n</code></pre>"},{"location":"1_3_docker_tp.html#53-get-the-algorithm-in-ml-git-in-your-virtual-machine","title":"5.3 Get the algorithm in ML git in your Virtual Machine","text":"<ul> <li>From your vm, run <code>git clone https://github.com/erachelson/MLclass.git</code>, this should setup your AML class inside your VM</li> </ul>"},{"location":"1_3_docker_tp.html#54-mounting-volumes-and-ports","title":"5.4 Mounting volumes and ports","text":"<p>Now let's run the image. This container has a jupyter notebook accessible from port 8080 so we will need to map the host port 8888 (the one accessible from the ssh tunnel) to the docker port 8080, we will use port forwarding</p> <p>We will also need to make available the notebooks on the VM to the container... we will mount volumes. Your data is located in <code>/home/${USER}/MLClass</code> and we want to miunt it in <code>/tmp/workdir</code></p> <pre><code>docker run --rm -it \\\n-p 8888:8888 \\\n-v /home/${USER}/MLclass:/home/jovyan/work/MLClass \\\n--workdir /home/jovyan/work \\\njupyter/scipy-notebook:lab-3.5.3\n</code></pre> <p>Note: this image is large, delete it afterwards using <code>docker rmi</code></p> <p>Options breakdown:</p> <ul> <li><code>--rm</code> remove the container when we stop it</li> <li><code>-it</code> run the container in interactive mode</li> <li><code>-p</code> forward port from host:container</li> <li>other: options from the kaggle container</li> </ul> <p>You should now see a jupyter lab with mlclass accessible if you do another port mapping</p> <p>So to connect to the jupyter lab we mapped the ports local 8888 to vm 8888 and vm 8888 to docker 8888</p> <p>We also exposed the local disk to the container</p>"},{"location":"1_3_docker_tp.html#6-bonus-using-google-cloud-tools-for-docker","title":"6. Bonus - Using Google Cloud Tools for Docker","text":"<p>Using codespace, you should be able to do the Hello World Dockerfile exercise except that instead of using docker build you use Google Cloud Build</p> <p>Tutorial: https://cloud.google.com/cloud-build/docs/quickstart-docker</p> <p>Example command :<code>gcloud builds submit --tag eu.gcr.io/$PROJECT_ID/{image}:{tag} .</code></p> <p>Help</p> <p>to get your project id: <code>PROJECT_ID=$(gcloud config get-value project 2&gt; /dev/null)</code></p> <p>Example</p> <p>Try to build the hello world app</p>"},{"location":"1_3_docker_tp.html#7-bonus-docker-compose","title":"7. Bonus - Docker Compose","text":"<p>https://hackernoon.com/practical-introduction-to-docker-compose-d34e79c4c2b6</p> <p>https://github.com/docker/labs/blob/master/beginner/chapters/votingapp.md</p>"},{"location":"1_3_docker_tp.html#8-bonus-going-further","title":"8. Bonus - Going further","text":"<p>https://container.training/</p>"},{"location":"1_4_be.html","title":"Bureau d'\u00e9tudes Cloud &amp; Docker","text":"<p>Link to slides</p> <p> </p>"},{"location":"1_4_be.html#objectives-of-this-be","title":"Objectives of this BE","text":"<p>This Bureau d'\u00e9tudes (BE, for short) will guide you through the essential notions to be able to manipulate with regard to cloud computer and docker,</p> <p>We will illustrate the following:</p> <ul> <li>Work in a remote environment (inside a VM, using google cloud shell)</li> <li>Creation and ssh connection to virtual machine instances</li> <li>Usage of managed storage capabilities</li> <li>Creating your own docker images</li> <li>Exchanging docker images through a Container Registry</li> <li>Pulling and running docker images created by your teammates</li> </ul> <p>In particular, this workflow:</p> <p></p> <p>Warning</p> <p>Please read all the text in the question before executing the step-by-step instructions because there might be help or indications after the instructions.</p>"},{"location":"1_4_be.html#how-to-run-this-be","title":"How to run this BE","text":"<p>The best way to run this BE is to setup a Github Codespace VM and install the google cloud sdk. Refer to the previous TP to learn more</p> <p>We will be using the <code>gcloud</code> CLI for the following:</p> <ul> <li>Create a GCE Virtual Machine</li> <li>Connect to SSH with port forwarding to said machine</li> </ul> <p>For the rest of this walkthrough, if it is written \"from your local machine\", this will be \"github codespace\"</p> <p>If it is written \"inside the VM\", this means that you should run it inside the GCE VM that you have to run the SSH tunnel first...</p> <p>\ud83d\ude4f\ud83c\udffb Use Google Chrome without any ad blockers if you have any issues</p> <p>Warning</p> <p>\u26a0\ufe0f ISAE-EDU is tricky and may prevent you from correctly connecting \u26a0\ufe0f eduroam works best with ssh connections</p>"},{"location":"1_4_be.html#team-composition","title":"Team composition","text":"<p>You should be in team of 5, however this will work with a minimum of 2 people. Designate a \"project manager\" (the person who is the most comfortable with the google cloud platform UI). She or He will have the hard task of giving access to his/her GCP project to the other team members to enable collaboration.</p> <p>This means that the project of the \"team leader\" will be billed a little more for the duration of this BE, so please be kind with the project and apply good cloud hygiene :)</p> <p>Each team member picks a different cute mascot and remembers it:</p> <ul> <li>\ud83d\udc08 cat</li> <li>\ud83d\udc15 dog</li> <li>\ud83d\udc7d (baby) yoda</li> <li>\ud83e\udd89 owl</li> <li>\ud83d\udc3c panda</li> </ul> <p>Find a groupname, because you will need it for the next steps</p>"},{"location":"1_4_be.html#0-setup","title":"0 - Setup","text":"<ul> <li>Launch your Github Codespaces instance</li> <li>Ensure that the google cloud sdk is installed and configured to the <code>isae-sdd</code> project</li> </ul>"},{"location":"1_4_be.html#1-get-the-necessary-resources-from-google-cloud-storage","title":"1 - Get the necessary resources from Google Cloud Storage","text":"<p>From your github codespace,</p> <p>The resources are located at the URI <code>gs://fchouteau-isae-cloud/be/${MASCOT}</code>,</p> <p>Your <code>${MASCOT}</code> name is either:</p> <ul> <li>cat</li> <li>dog</li> <li>owl</li> <li>panda</li> <li>yoda</li> </ul> <p>I advise you to <code>export MASCOT=....</code> to remember it :)</p> <p>ONLY DOWNLOAD your mascot resources (no cheating ! this will only cause confusion later)</p> <p>Download them to your instance using the gcloud cli (refer to your previous work for more information)</p> Hint <p><pre><code>gsutil -m cp -r {source} {destination}\n</code></pre> Remember that google storage URIs always begin with gs://</p> <p>Go to (<code>cd</code>) the folder where you downloaded your resources</p> <p>You should see a file structure like this</p> <pre><code>fchouteau@be-cloud-mascot:~/be$ tree yoda  -L 2\nyoda\n\u251c\u2500\u2500 app.py\n\u251c\u2500\u2500 AUTHOR.txt\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 favicon.ico\n\u251c\u2500\u2500 imgs\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 1.gif\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 2.gif\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 3.gif\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 4.gif\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 5.gif\n\u2514\u2500\u2500 template.html.jinja2\n\n1 directory, 10 files\n</code></pre>"},{"location":"1_4_be.html#2-build-your-docker-image","title":"2 - Build your docker image","text":"<p>Question</p> <ul> <li>Look at the <code>Dockerfile</code> (<code>cat Dockerfile</code>), what does it seem to do ?</li> <li>Look at <code>app.py</code> (<code>cat app.py</code>). What is Flask ? What does it seem to do ?</li> </ul> <ul> <li>Edit the file <code>AUTHOR.txt</code> to add your name instead of the placeholder</li> <li>Refer to your previous work to build the image</li> </ul> <p>Danger</p> <p>On which port is your flask app running ? (<code>cat Dockerfile</code>) Note it carefully ! You will need to communicate it to your teammate :)</p> <ul> <li>When building the image, name it appropriately... like <code>eu.gcr.io/${PROJECT_ID}/webapp-gif:${GROUPNAME}-${MASCOT}-1.0</code> !</li> </ul> Hint <p>to get your project id: <pre><code>PROJECT_ID=$(gcloud config get-value project 2&gt; /dev/null)\n</code></pre></p> <ul> <li>now if you list your images you should see it !</li> </ul> <pre><code>REPOSITORY                                      TAG                 IMAGE ID            CREATED             SIZE\neu.gcr.io/{your project}/{your-app}    1.0                 d1c5993848bf        2 minutes ago       62.1MB\n</code></pre> <p>Question</p> <p>Describe concisely (on slack) to your past self (before the pandemic) what is a <code>Docker Image</code></p>"},{"location":"1_4_be.html#3-push-your-docker-image-in-the-container-registry","title":"3 - Push your Docker image in the Container Registry","text":"<ul> <li> <p>Now push your image on the shared container registry</p> </li> <li> <p>Help your team mates so that everybody can build his/her Docker Image</p> </li> </ul> <p>Question</p> <p>Describe succintly (on slack) to your past self (before the pandemic) what is a <code>Container Registry</code></p> <p>Bug</p> <p>in case of error</p> <pre><code>gcloud auth configure-docker\n</code></pre> <p>In the end, things should look like this</p> <p></p>"},{"location":"1_4_be.html#4-create-google-compute-engine-vm","title":"4 - Create Google Compute Engine VM","text":"<p>Each team member creates a separate machine on the same project,</p> <p>Here, you will create a Google Compute Engine instance, preconfigured with everything you need,</p> <p>If you use the google cloud CLI (either your local google cloud sdk or google cloud shell), you can use this</p> <p>First, set a variable with the name of your instance,</p> <pre><code>export INSTANCE_NAME=\"be-cloud-mascot-{yourgroup}-{yourname}\" # Don't forget to replace values !\n</code></pre> <p>Then create your VM</p> <pre><code>gcloud compute instances create $INSTANCE_NAME \\\n--zone=\"europe-west1-b\" \\\n--machine-type=\"n1-standard-1\" \\\n--image-family=\"common-cpu\" \\\n--image-project=\"deeplearning-platform-release\" \\\n--maintenance-policy=TERMINATE \\\n--scopes=\"storage-rw\" \\\n--boot-disk-size=75GB\n</code></pre> <p>If you have an issue with quota, use any of <code>europe-west4-{a,b,c,d}</code> or <code>europe-west1-{b,c,d}</code> or <code>europe-2-{a,b,c,d}</code> as a zone</p> <p>If you use the web interface, follow this</p>  Your browser does not support the video tag.  <p>Question</p> <p>Describe concisely (on slack) to your past self (before the pandemic) what is a <code>Virtual Machine</code> and what is <code>Google Compute Engine</code></p>"},{"location":"1_4_be.html#5-connect-using-ssh-to-the-instance","title":"5 - Connect using SSH to the instance","text":"<p>If you are using the google cloud sdk in your computer or github codespace, you can connect to ssh using the usual command (refer to the first hands-on) with SSH Tunneling. FIRST CONNECT TO EDUROAM as eduroam allows connecting to ssh.</p> <p>Tunnel the following ports to your local machine:</p> <ul> <li>8080: This is reserved for a jupyter lab session by default, it makes it easy to see &amp; edit text</li> <li>8081: You will neeed to run containers and expose them on a port</li> </ul> Hint <pre><code>gcloud compute ssh {user}@{instance} -- \\\n-L {client-port}:localhost:{server-port} \\\n-L {client-port-2}:localhost:{server-port-2}\n</code></pre> <p>Go to your browser and connect to http://localhost:8080, you should be in a jupyter lab where you can access a terminal, a text editor etc...</p> <p>Question</p> <p>Where is this jupyter lab hosted ? Describe concisely what is a SSH Tunnel and what is port forwarding</p>"},{"location":"1_4_be.html#6-pull-docker-images-from-your-teammates","title":"6 - Pull Docker Images from your teammates","text":"<p>You should be inside the your VM,</p> <p>Question</p> <p>How to check that you're inside your VM ? On your terminal you should see user@hostname at the beginning. Hostname should be the name of your VM</p> <ul> <li> <p>Select another mascot and pull the corresponding docker image from the registry</p> </li> <li> <p>List the docker images you have. You should have at least 2 including yours</p> </li> </ul>"},{"location":"1_4_be.html#7-run-docker-containers-from-their-docker-images","title":"7 - Run Docker Containers from their Docker Images","text":"<ul> <li> <p>Run your container while mapping the correct port to your VM 8081. Which port is it ? Well, ask the one who built the image.</p> </li> <li> <p>When running the container, setup the <code>USER</code> environment variable to your name !</p> </li> </ul> <p>Hint</p> <p>the port is not the same as yours if you don't set the username, it will come to bite your later ;)</p>"},{"location":"1_4_be.html#9-display-the-results-share-them","title":"9 - Display the results &amp; share them","text":"<ul> <li> <p>You just launched a webapp on the port 8081 of your remote instance.</p> </li> <li> <p>If you have a ssh tunnel directly from your laptop, ensure that you made a tunnel for your port 8081 to any port of your machine then, go to <code>http://localhost:(your port)</code> inside your browser. The resulting webpage should appear</p> </li> <li> <p>If you are using github codespace, open web preview on port 8081 (you should have a tunnel running between your github codespace and your GCE instance)</p> </li> <li> <p>The 8081 port has been opened to the internet, you can also connect to your machine's public ip address on the port 8081 (http//{ip}:8081) and you should see it.</p> </li> </ul> <p>How to get your public IP ? Go to the GCP interface and you can find it, or run <code>gcloud compute instances list | grep {your instance name}</code> </p> <p>Note : This only works in 4G so use your mobile phone to check for the website</p> <p>Success</p> <ul> <li>The webpage should show the mascot your chose to run  </li> <li>The webpage should show the name of the author (not you)</li> <li>The webpage should show your name</li> </ul> <p>Bug</p> <p>If any of the three item above are missing, find the bug and solve it :)</p> <p>Example</p> <p>Try to refresh the webpage to make more gifs appear</p> <p>Share your result on slack</p>"},{"location":"1_4_be.html#10-cleanup-the-gcp-project","title":"10. Cleanup the GCP project","text":"<ul> <li>Remove your VMs (DELETE them)</li> <li>Remove images from the container registry</li> </ul>"},{"location":"1_4_be.html#11-yay","title":"11. Yay !","text":"<p>Success</p> <p>\ud83c\udf89 you have successfully finished the BE. You know how to manipulate the basic notions around cloud computing and docker so that you won't be completely lost when someone will talk about it</p> <p>If you have time left, you can go back and finish all the previous hands-on</p> <p>You can also look at the Streamlit Hands-On by Toulouse Data Science in preparation for the next class : - Slides - Repository</p> <p>If you have even more time, you can have a look at the next class, do the hands-on, and if you really have finished everything, read the kubernetes comic and do the Kubernetes hands-on !</p>"},{"location":"1_5_deployment.html","title":"Data Computation applied to Artificial Intelligence","text":""},{"location":"1_5_deployment.html#intro-to-mlops-deployment","title":"Intro to MLOPs &amp; Deployment","text":"<p>Link to slides</p> <p> </p>"},{"location":"1_5_deployment.html#intro-to-orchestration","title":"Intro to Orchestration","text":"<p>Link to slides</p> <p> </p>"},{"location":"1_5_deployment_tp.html","title":"Deployment : Deploy your ML model in production","text":""},{"location":"1_5_deployment_tp.html#objectives","title":"Objectives","text":"<p>Your first ML model in production !</p> <ul> <li>A model behind a Restful API, packaged in a docker</li> <li>A frontend using streamlit, packaged in a docker</li> <li>Deploy a multi-container application using docker compose</li> <li>Deploy the model in the docker image</li> <li>Send it to your friends !</li> </ul> <p>Regardons ce notebook</p> <p>Il effectue les op\u00e9rations suivantes:</p> <ul> <li>Chargement d'un mod\u00e8le</li> <li>Chargement d'une image</li> <li>D\u00e9tection des \"objets\" sur l'image</li> <li>Dessin des d\u00e9tections sur l'image</li> <li>Affichage</li> </ul> <p>L'objectif est de convertir ce notebook en deux applications :</p> <ul> <li>L'une qui \"sert\" les pr\u00e9dictions d'un mod\u00e8le (le serveur)</li> <li>L'une qui permet \u00e0 un utilisateur d'interagir facilement avec le mod\u00e8le en mettant en ligne sa propre image (le \"client\")</li> </ul> <p>Nous allons d\u00e9velopper tout cela dans l'environnement de d\u00e9veloppement (codespaces)</p> <p>Puis d\u00e9ployer le mod\u00e8le dans l'environnement GCP</p>"},{"location":"1_5_deployment_tp.html#team-composition","title":"Team Composition","text":"<p>C'est mieux d'\u00eatre en bin\u00f4me pour s'entraider :)</p>"},{"location":"1_5_deployment_tp.html#configuration-du-codespace","title":"Configuration du codespace","text":"<p>Nous allons utiliser github codespaces comme environnement de d\u00e9veloppement,</p> <p>Repartir de https://github.com/github/codespaces-blank</p> <p>Puis configurer ce codespace avec le google cloud sdk et configurer le projet <code>isae-sdd</code></p> <p>Hint</p> <pre><code># Rappels : Installation du google cloud sdk\n# https://cloud.google.com/sdk/docs/install#linux\ncurl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-cli-416.0.0-linux-x86_64.tar.gz\ntar -xf google-cloud-cli-416.0.0-linux-x86.tar.gz\n./google-cloud-sdk/install.sh\n# Type yes to add to path !\nexport PATH=./google-cloud-sdk/bin:$PATH\ngcloud init\n# login and copy the token\n# configure isae-sdd then compute zone 17\ngcloud auth configure-docker\n</code></pre> <p>Voir les tps pr\u00e9c\u00e9dents</p> <p>Maintenant, depuis ce codespace, ouvrez un terminal et r\u00e9cup\u00e9rez les fichiers suivants :</p> <pre><code>gsutil cp -r gs://fchouteau-isae-cloud/deployment/* .\n</code></pre> <p>Hint</p> <p>Si vous tombez \u00e0 court de stockage dans le TP, lancez <code>docker system prune</code> pour nettoyer le cache docker</p>"},{"location":"1_5_deployment_tp.html#1-converting-a-prediction-notebook-into-a-webapplication","title":"1 - Converting a prediction notebook into a webapplication","text":"<p>Placez vous dans le dossier <code>model</code> nouvellement cr\u00e9\u00e9</p>"},{"location":"1_5_deployment_tp.html#objectif","title":"Objectif","text":"<p>Packager un mod\u00e8le de machine learning derri\u00e8re une webapplication pour pouvoir la d\u00e9ployer sur le web et servir des pr\u00e9dictions \u00e0 des utilisateurs</p> <p>Le mod\u00e8le: Un d\u00e9tecteur d'objets sur des photographies \"standard\" suppos\u00e9 marcher en temps r\u00e9el, qui sort des \"bounding boxes\" autour des objets d\u00e9tect\u00e9 dans des images</p> <p></p> <p>Remarque : Le papier vaut la lecture https://pjreddie.com/media/files/papers/YOLOv3.pdf</p> <p>On r\u00e9cup\u00e8re la version disponible sur torchhub https://pytorch.org/hub/ultralytics_yolov5/ qui correspond au repository suivant https://github.com/ultralytics/yolov5</p> <p>Voici une petite explication de l'historique de YOLO https://medium.com/towards-artificial-intelligence/yolo-v5-is-here-custom-object-detection-tutorial-with-yolo-v5-12666ee1774e</p> <p>On se propose ici d'encapsuler 3 versions du mod\u00e8le (S,M,L) qui sont 3 versions +/- complexes du mod\u00e8le YOLO-V5, afin de pouvoir comparer les performances et les r\u00e9sultats</p> <p></p>"},{"location":"1_5_deployment_tp.html#deroulement","title":"D\u00e9roulement","text":"<ul> <li>Transformer un notebook de pr\u00e9diction en \u201cWebApp\u201d en remplissant <code>app.stub.py</code> et en le renommant en <code>app.py</code></li> <li>Packager l'application sous forme d'une image docker</li> <li>Tester son image docker localement</li> <li>Uploader le docker sur Google Container Registry</li> </ul>"},{"location":"1_5_deployment_tp.html#developpement-de-apppy","title":"D\u00e9veloppement de app.py","text":"<p>Regardons le <code>app.stub.py</code> (que l'on renommera en <code>app.py</code>)</p> <pre><code>import base64\nimport io\nimport time\nfrom typing import List, Dict\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\n\nclass Input(BaseModel):\n    model: str\n    image: str\n\n\nclass Detection(BaseModel):\n    x_min: int\n    y_min: int\n    x_max: int\n    y_max: int\n    class_name: str\n    confidence: float\n\n\nclass Result(BaseModel):\n    detections: List[Detection] = []\n    time: float = 0.0\n    model: str\n\n\n# !!!! FILL ME\ndef parse_predictions(prediction: np.ndarray, classes: [str]) -&gt; List[Detection]:\n    raise NotImplementedError\n\n\n# !!!! FILL ME\ndef load_model(model_name: str):\n\"\"\"\"\"\"\n    raise NotImplementedError\n\n\nMODEL_NAMES = [\"yolov5s\", \"yolov5m\", \"yolov5l\"]\n\napp = FastAPI(\n    title=\"NAME ME\",\n    description=\"\"\"\n                DESCRIBE ME\n                \"\"\",\n    version=\"1.0\",\n)\n\n# !!!! FILL ME\n# This is a dictionnary that must contains a model for each key (model names), fill load model\n# example: for model_name in MODEL_NAMES: MODELS[model_name] = load_model(model_name)\n# You can also lazily load models only when they are called to avoid holding 3 models in memory\nMODELS = ...\n\n\n@app.get(\"/\", description=\"return the title\", response_description=\"FILL ME\", response_model=str)\ndef root() -&gt; str:\n    return app.title\n\n\n@app.get(\"/describe\", description=\"FILL ME\", response_description=\"FILL ME\", response_model=str)\ndef describe() -&gt; str:\n    return app.description\n\n\n@app.get(\"/health\", description=\"FILL ME\", response_description=\"FILL ME\", response_model=str)\ndef health() -&gt; str:\n    return \"HEALTH OK\"\n\n\n@app.get(\"/models\", description=\"FILL ME\", response_description=\"FILL ME\", response_model=List[str])\ndef models() -&gt; [str]:\n    return MODEL_NAMES\n\n\n@app.post(\"/predict\", description=\"FILL ME\", response_description=\"FILL ME\", response_model=Result)\ndef predict(inputs: Input) -&gt; Result:\n\n    # get correct model\n    model_name = inputs.model\n\n    if model_name not in MODEL_NAMES:\n        raise HTTPException(status_code=400, detail=\"wrong model name, choose between {}\".format(MODEL_NAMES))\n\n    # Get the model from the list of available models\n    model = MODELS.get(model_name)\n\n    # Get &amp; Decode image\n    try:\n        image = inputs.image.encode(\"utf-8\")\n        image = base64.b64decode(image)\n        image = Image.open(io.BytesIO(image))\n    except:\n        raise HTTPException(status_code=400, detail=\"File is not an image\")\n    # Convert from RGBA to RGB *to avoid alpha channels*\n    if image.mode == \"RGBA\":\n        image = image.convert(\"RGB\")\n\n    # Inference\n\n    # RUN THE PREDICTION, TIME IT\n    predictions = ...\n\n    # Post processing\n    classes = predictions.names\n    predictions = predictions.xyxy[0].numpy()\n\n    # Create a list of [DETECTIONS] objects that match the detection class above, using the parse_predictions method\n    detections = ...\n\n    result = Result(detections=..., time=..., model=...)\n\n    return result\n</code></pre> <p>Dans un premier temps, vous pouvez remplir la description des \"routes\" (i.e. des fonctions de l'application):</p> <pre><code>@app.get(\"/\", description=\"return the title\", response_description=\"FILL ME\", response_model=str)\ndef root() -&gt; str:\n    return app.title\n\n\n@app.get(\"/describe\", description=\"FILL ME\", response_description=\"FILL ME\", response_model=str)\ndef describe() -&gt; str:\n    return app.description\n\n\n@app.get(\"/health\", description=\"FILL ME\", response_description=\"FILL ME\", response_model=str)\ndef health() -&gt; str:\n    return \"HEALTH OK\"\n\n\n@app.get(\"/models\", description=\"FILL ME\", response_description=\"FILL ME\", response_model=List[str])\ndef models() -&gt; [str]:\n    return MODEL_NAMES\n</code></pre> <p>Il y a deux fonctions \u00e0 compl\u00e9ter en s'inspirant du notebook <code>inference.ipynb</code>. Grace au typage de python, vous avez les types d'entr\u00e9e et de sortie des deux fonctions</p> <p>La premi\u00e8re prend un tableau de type (left, top, right, bottom, confidence, class_index) et une liste de noms de classes et cr\u00e9\u00e9e une liste d'objets <code>Detection</code> (voir le code pour la cr\u00e9ation des objets d\u00e9tection)</p> <pre><code># !!!! FILL ME\ndef parse_predictions(predictions: np.ndarray, classes: [str]) -&gt; List[Detection]:\n    raise NotImplementedError\n</code></pre> Hint <pre><code>def parse_prediction(prediction: np.ndarray, classes: [str]) -&gt; Detection:\nx0, y0, x1, y1, cnf, cls = prediction\ndetection = Detection(\n    x_min=int(x0),\n    y_min=int(y0),\n    x_max=int(x1),\n    y_max=int(y1),\n    confidence=round(float(cnf), 3),\n    class_name=classes[int(cls)],\n)\nreturn detection\n</code></pre> <p>La seconde fonction doit charger un mod\u00e8le via torchhub en fonction de son nom (voir le docker)</p> <pre><code># !!!! FILL ME\ndef load_model(model_name: str):\n\"\"\"\"\"\"\n    raise NotImplementedError\n</code></pre> Hint <pre><code>def load_model(model_name: str) -&gt; Dict:\n    # Load model from torch\n    model = torch.hub.load(\"ultralytics/yolov5\", model_name, pretrained=True)\n    # Evaluation mode + Non maximum threshold\n    model = model.eval()\n\nreturn model\n</code></pre> <p>Ensuite, vous pouvez executer les fonctions de chargement de mod\u00e8le, par exemple</p> <pre><code># !!!! FILL ME\n# This is a dictionnary that must contains a model for each key (model names), fill load model\n# example: for model_name in MODEL_NAMES: MODELS[model_name] = load_model(model_name)\n# You can also lazily load models only when they are called to avoid holding 3 models in memory\nMODELS = {}\nfor model_name in MODEL_NAMES:\n    MODELS[model_name] = load_model(model_name)\n</code></pre> <p>Enfin, il s'agit d'\u00e9crire un code qui effectue une pr\u00e9diction \u00e0 partir d'une image PIL et de mesurer le temps (indice: <code>import time</code> et <code>t0 = time.time()</code> ...) de pr\u00e9diction</p> <pre><code># RUN THE PREDICTION, TIME IT\npredictions = ...\n# Post processing\nclasses = predictions.names\npredictions = predictions.xyxy[0].numpy()\n</code></pre> <p>Le r\u00e9sultat de predictions est un tableau numpy compos\u00e9 des colonnes <code>left, top, right, bottom, confidence, class_index</code></p> <p>Il s'agit ensuite de transformer ces predictions en <code>[Detection]</code></p> <pre><code>class Detection(BaseModel):\n    x_min: int\n    y_min: int\n    x_max: int\n    y_max: int\n    class_name: str\n    confidence: float\n</code></pre> <pre><code># Create a list of [DETECTIONS] objects that match the detection class above, using the parse_predictions method\ndetections = parse_predictions(predictions, classes)\n</code></pre> Hint <pre><code># Inference\nt0 = time.time()\npredictions = model(image, size=640)  # includes NMS\nt1 = time.time()\nclasses = predictions.names\n\n# Post processing\npredictions = predictions.xyxy[0].numpy()\ndetections = [parse_prediction(prediction=pred, classes=classes) for pred in predictions]\n\nresult = Result(detections=detections, time=round(t1 - t0, 3), model=model_name)\n</code></pre>"},{"location":"1_5_deployment_tp.html#correction","title":"Correction","text":"<p><code>app.py</code></p> Hint <pre><code>import base64\nimport io\nimport time\nfrom typing import List, Dict\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n\n\nclass Input(BaseModel):\n    model: str\n    image: str\n\n\nclass Detection(BaseModel):\n    x_min: int\n    y_min: int\n    x_max: int\n    y_max: int\n    class_name: str\n    confidence: float\n\n\nclass Result(BaseModel):\n    detections: List[Detection] = []\n    time: float = 0.0\n    model: str\n\n\ndef parse_prediction(prediction: np.ndarray, classes: [str]) -&gt; Detection:\n    x0, y0, x1, y1, cnf, cls = prediction\n    detection = Detection(\n        x_min=int(x0),\n        y_min=int(y0),\n        x_max=int(x1),\n        y_max=int(y1),\n        confidence=round(float(cnf), 3),\n        class_name=classes[int(cls)],\n    )\n    return detection\n\n\ndef load_model(model_name: str) -&gt; Dict:\n    # Load model from torch\n    model = torch.hub.load(\"ultralytics/yolov5\", model_name, pretrained=True)\n    # Evaluation mode + Non maximum threshold\n    model = model.eval()\n\n    return model\n\n\n# %%\napp = FastAPI(\n    title=\"YOLO-V5 WebApp created with FastAPI\",\n    description=\"\"\"\n                Wraps 3 different yolo-v5 models under the same RESTful API\n                \"\"\",\n    version=\"1.1\",\n)\n\n# %%\nMODEL_NAMES = [\"yolov5s\", \"yolov5m\", \"yolov5l\"]\nMODELS = {}\n\n\n@app.get(\"/\", description=\"return the title\", response_description=\"title\", response_model=str)\ndef root() -&gt; str:\n    return app.title\n\n\n@app.get(\"/describe\", description=\"return the description\", response_description=\"description\", response_model=str)\ndef describe() -&gt; str:\n    return app.description\n\n\n@app.get(\"/version\", description=\"return the version\", response_description=\"version\", response_model=str)\ndef describe() -&gt; str:\n    return app.version\n\n\n@app.get(\"/health\", description=\"return whether it's alive\", response_description=\"alive\", response_model=str)\ndef health() -&gt; str:\n    return \"HEALTH OK\"\n\n\n@app.get(\n    \"/models\",\n    description=\"Query the list of models\",\n    response_description=\"A list of available models\",\n    response_model=List[str],\n)\ndef models() -&gt; [str]:\n    return MODEL_NAMES\n\n\n@app.post(\n    \"/predict\",\n    description=\"Send a base64 encoded image + the model name, get detections\",\n    response_description=\"Detections + Processing time\",\n    response_model=Result,\n)\ndef predict(inputs: Input) -&gt; Result:\n    global MODELS\n\n    # get correct model\n    model_name = inputs.model\n\n    if model_name not in MODEL_NAMES:\n        raise HTTPException(status_code=400, detail=\"wrong model name, choose between {}\".format(MODEL_NAMES))\n\n    # check load\n    if MODELS.get(model_name) is None:\n        MODELS[model_name] = load_model(model_name)\n\n    model = MODELS.get(model_name)\n\n    # Get Image\n    # Decode image\n    try:\n        image = inputs.image.encode(\"utf-8\")\n        image = base64.b64decode(image)\n        image = Image.open(io.BytesIO(image))\n    except:\n        raise HTTPException(status_code=400, detail=\"File is not an image\")\n    # Convert from RGBA to RGB *to avoid alpha channels*\n    if image.mode == \"RGBA\":\n        image = image.convert(\"RGB\")\n\n    # Inference\n    t0 = time.time()\n    predictions = model(image, size=640)  # includes NMS\n    t1 = time.time()\n    classes = predictions.names\n\n    # Post processing\n    predictions = predictions.xyxy[0].numpy()\n    detections = [parse_prediction(prediction=pred, classes=classes) for pred in predictions]\n\n    result = Result(detections=detections, time=round(t1 - t0, 3), model=model_name)\n\n    return result\n</code></pre>"},{"location":"1_5_deployment_tp.html#construire-le-docker","title":"Construire le docker","text":"<pre><code>PROJECT_ID=$(gcloud config get-value project 2&gt; /dev/null)\ndocker build -t eu.gcr.io/${PROJECT_ID}/{you rname}{your app name}:{your version} -f Dockerfile . </code></pre>"},{"location":"1_5_deployment_tp.html#tester-le-docker","title":"Tester le docker","text":"<p>Vous pouvez lancer le docker localement et le tester  avec le notebook</p> <pre><code>PROJECT_ID=$(gcloud config get-value project 2&gt; /dev/null)\ndocker run --rm -p 8000:8000 eu.gcr.io/${PROJECT_ID}/{your-name}-{your app name}:{your version}\n</code></pre> <p>Vous pouvez vous connecter \u00e0 votre appli via son ip publique sur le port 8000 depuis votre navigateur local</p> <p><code>http://{ip}:8000</code></p> <p>Essayez quelques routes :</p> <p><code>/models</code> <code>/docs</code></p>"},{"location":"1_5_deployment_tp.html#pusher-le-docker-sur-google-container-registry","title":"Pusher le docker sur google container registry","text":"<pre><code>gcloud auth configure-docker\ndocker push eu.gcr.io/${PROJECT_ID}/{your-name}-model:{your version}\n</code></pre> <p>Si vous devez mettre \u00e0 jour le docker, il faut incr\u00e9menter la version pour le d\u00e9ploiement</p>"},{"location":"1_5_deployment_tp.html#liens-utiles","title":"Liens Utiles","text":"<ul> <li>https://fastapi.tiangolo.com/</li> <li>https://requests.readthedocs.io/en/master/</li> <li>https://testdriven.io/blog/fastapi-streamlit/</li> </ul>"},{"location":"1_5_deployment_tp.html#2-making-a-companion-application","title":"2 - Making a companion application","text":"<p>Allez dans le dossier <code>streamlit</code></p>"},{"location":"1_5_deployment_tp.html#objectif_1","title":"Objectif","text":"<p>Cr\u00e9er une application \"compagnon\" qui permet de faire des requ\u00eates \u00e0 un mod\u00e8le de fa\u00e7on ergonomique et de visualiser les r\u00e9sultats</p>"},{"location":"1_5_deployment_tp.html#deroulement_1","title":"D\u00e9roulement","text":"<ul> <li>Remplir <code>app.stub.py</code>, le renommer en <code>app.py</code> en remplissant les bons champs (s'aider des notebooks dans <code>app/</code>) et en cr\u00e9ant des jolies visualisations</li> <li>Packager l'application sous forme d'une image docker</li> <li>Tester son image docker localement</li> <li>Uploader le docker sur Google Container Registry</li> </ul>"},{"location":"1_5_deployment_tp.html#guide-de-developpement","title":"Guide de d\u00e9veloppement","text":"<p>Regardons le <code>APP.md</code></p> <ul> <li>Remplissez le fichier avec la description de votre application</li> </ul> <p>Regardons le <code>app.stub.py</code> </p> <pre><code>import requests\nimport streamlit as st\nfrom PIL import Image\nimport io\nimport base64\nfrom pydantic import BaseModel\nfrom typing import List\nimport random\n\n# ---- Functions ---\n\n\nclass Detection(BaseModel):\n    x_min: int\n    y_min: int\n    x_max: int\n    y_max: int\n    class_name: str\n    confidence: float\n\n\nclass Result(BaseModel):\n    detections: List[Detection] = []\n    time: float = 0.0\n    model: str\n\n\n@st.cache(show_spinner=True)\ndef make_dummy_request(model_url: str, model: str, image: Image) -&gt; Result:\n\"\"\"\n    This simulates a fake answer for you to test your application without having access to any other input from other teams\n    \"\"\"\n    # We do a dummy encode and decode pass to check that the file is correct\n    with io.BytesIO() as buffer:\n        image.save(buffer, format=\"PNG\")\n        buffer: str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        data = {\"model\": model, \"image\": buffer}\n\n    # We do a dummy decode\n    _image = data.get(\"image\")\n    _image = _image.encode(\"utf-8\")\n    _image = base64.b64decode(_image)\n    _image = Image.open(io.BytesIO(_image))  # type: Image\n    if _image.mode == \"RGBA\":\n        _image = _image.convert(\"RGB\")\n\n    _model = data.get(\"model\")\n\n    # We generate a random prediction\n    w, h = _image.size\n\n    detections = [\n        Detection(\n            x_min=random.randint(0, w // 2 - 1),\n            y_min=random.randint(0, h // 2 - 1),\n            x_max=random.randint(w // w, w - 1),\n            y_max=random.randint(h // 2, h - 1),\n            class_name=\"dummy\",\n            confidence=round(random.random(), 3),\n        )\n        for _ in range(random.randint(1, 10))\n    ]\n\n    # We return the result\n    result = Result(time=0.1, model=_model, detections=detections)\n\n    return result\n\n\n@st.cache(show_spinner=True)\ndef make_request(model_url: str, model: str, image: Image) -&gt; Result:\n\"\"\"\n    Process our data and send a proper request\n    \"\"\"\n    with io.BytesIO() as buffer:\n        image.save(buffer, format=\"PNG\")\n        buffer: str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        data = {\"model\": model, \"image\": buffer}\n\n        response = requests.post(\"{}/predict\".format(model_url), json=data)\n\n    if not response.status_code == 200:\n        raise ValueError(\"Error in processing payload, {}\".format(response.text))\n\n    response = response.json()\n\n    return Result.parse_obj(response)\n\n\n# ---- Streamlit App ---\n\nst.title(\"NAME ME BECAUSE I AM AWESOME\")\n\nwith open(\"APP.md\") as f:\n    st.markdown(f.read())\n\n# --- Sidebar ---\n# defines an h1 header\n\nmodel_url = st.sidebar.text_input(label=\"Cluster URL\", value=\"http://localhost:8000\")\n\n_model_url = model_url.strip(\"/\")\n\nif st.sidebar.button(\"Send 'is alive' to IP\"):\n    try:\n        response = requests.get(\"{}/health\".format(_model_url))\n        if response.status_code == 200:\n            st.sidebar.success(\"Webapp responding at {}\".format(_model_url))\n        else:\n            st.sidebar.error(\"Webapp not respond at {}, check url\".format(_model_url))\n    except ConnectionError:\n        st.sidebar.error(\"Webapp not respond at {}, check url\".format(_model_url))\n\ntest_mode_on = st.sidebar.checkbox(label=\"Test Mode - Generate dummy answer\", value=False)\n\n# --- Main window\n\nst.markdown(\"## Inputs\")\nst.markdown(\"Describe something... You can also add things like confidence slider etc...\")\n\n# Here we should be able to choose between [\"yolov5s\", \"yolov5m\", \"yolov5l\"], perhaps a radio button with the three choices ?\nmodel_name = ...\n\n# Here we should be able to upload a file (our image)\nimage_file = ...\n\n# Converting image, this is done for you :)\nif image_file is not None:\n    image_file.seek(0)\n    image = image_file.read()\n    image = Image.open(io.BytesIO(image))\n\nif st.button(label=\"SEND PAYLOAD\"):\n\n    if test_mode_on:\n        st.warning(\"Simulating a dummy request to {}\".format(model_url))\n        result = ...  # call the proper function\n    else:\n        result = ...  # call the proper function\n\n    st.balloons()\n\n    st.markdown(\"## Display\")\n\n    st.markdown(\"Make something pretty, draw polygons and confidence..., here's an ugly output\")\n\n    st.image(image, width=512, caption=\"Uploaded Image\")\n\n    st.text(\"Model : {}\".format(result.model))\n    st.text(\"Processing time : {}s\".format(result.time))\n\n    for detection in result.detections:\n        st.json(detection.json())\n</code></pre> <p>La majorit\u00e9 des fonctions de requ\u00eate sont d\u00e9j\u00e0 impl\u00e9ment\u00e9es, il reste \u00e0 faire les fonctions d'entr\u00e9es utilisateurs et la visualisation</p> <ul> <li>Entr\u00e9e: Utilisation de <code>st.radio</code> et <code>st.file_uploader</code>: </li> </ul> <p>https://docs.streamlit.io/en/stable/getting_started.html</p> <p>https://docs.streamlit.io/en/stable/api.html#streamlit.radio</p> <p>https://docs.streamlit.io/en/stable/api.html#streamlit.file_uploader</p> <pre><code>st.markdown(\"## Inputs\")\nst.markdown(\"Select your model (Small, Medium or Large)\")\n\nmodel_name = st.radio(label=\"Model Name\", options=[\"yolov5s\", \"yolov5m\", \"yolov5l\"])\n\nst.markdown(\"Upload an image\")\n\nimage_file = st.file_uploader(label=\"Image File\", type=[\"png\", \"jpg\", \"tif\"])\n</code></pre> <ul> <li>Visualisations</li> </ul> <p>Exemple de code qui imite le notebook de pr\u00e9diction pour dessiner sur une image PIL</p> <pre><code>def draw_preds(image: Image, detections: [Detection]):\n\n    class_names = list(set([detection.class_name for detection in detections]))\n\n    image_with_preds = image.copy()\n\n    # Define colors\n    colors = plt.cm.get_cmap(\"viridis\", len(class_names)).colors\n    colors = (colors[:, :3] * 255.0).astype(np.uint8)\n\n    # Define font\n    font = list(Path(\"/usr/share/fonts\").glob(\"**/*.ttf\"))[0].name\n    font = ImageFont.truetype(font=font, size=np.floor(3e-2 * image_with_preds.size[1] + 0.5).astype(\"int32\"))\n    thickness = (image_with_preds.size[0] + image_with_preds.size[1]) // 300\n\n    # Draw detections\n    for detection in detections:\n        left, top, right, bottom = detection.x_min, detection.y_min, detection.x_max, detection.y_max\n        score = float(detection.confidence)\n        predicted_class = detection.class_name\n        class_idx = class_names.index(predicted_class)\n\n        label = \"{} {:.2f}\".format(predicted_class, score)\n\n        draw = ImageDraw.Draw(image_with_preds)\n        label_size = draw.textsize(label, font)\n\n        top = max(0, np.floor(top + 0.5).astype(\"int32\"))\n        left = max(0, np.floor(left + 0.5).astype(\"int32\"))\n        bottom = min(image_with_preds.size[1], np.floor(bottom + 0.5).astype(\"int32\"))\n        right = min(image_with_preds.size[0], np.floor(right + 0.5).astype(\"int32\"))\n\n        if top - label_size[1] &gt;= 0:\n            text_origin = np.array([left, top - label_size[1]])\n        else:\n            text_origin = np.array([left, top + 1])\n\n        # My kingdom for a good redistributable image drawing library.\n        for r in range(thickness):\n            draw.rectangle([left + r, top + r, right - r, bottom - r], outline=tuple(colors[class_idx]))\n        draw.rectangle([tuple(text_origin), tuple(text_origin + label_size)], fill=tuple(colors[class_idx]))\n\n        if any(colors[class_idx] &gt; 128):\n            fill = (0, 0, 0)\n        else:\n            fill = (255, 255, 255)\n\n        draw.text(text_origin, label, fill=fill, font=font)\n\n        del draw\n\n    return image_with_preds\n</code></pre> <p>Utilisation (exemple)</p> <pre><code>    if test_mode_on:\n        st.warning(\"Simulating a dummy request to {}\".format(model_url))\n        result = ...  # call the proper function\n    else:\n        result = ...  # call the proper function\n\n    st.balloons()\n\n    st.markdown(\"## Display\")\n\n    st.text(\"Model : {}\".format(result.model))\n    st.text(\"Processing time : {}s\".format(result.time))\n\n    image_with_preds = draw_preds(image, result.detections)\n    st.image(image_with_preds, width=1024, caption=\"Image with detections\")\n\n    st.markdown(\"### Detection dump\")\n    for detection in result.detections:\n        st.json(detection.json())\n</code></pre>"},{"location":"1_5_deployment_tp.html#corection-apppy","title":"Corection <code>app.py</code>","text":"Hint <pre><code>import base64\nimport io\nimport random\nfrom pathlib import Path\nfrom typing import List\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport requests\nimport streamlit as st\nfrom PIL import Image\nfrom PIL import ImageDraw, ImageFont\nfrom pydantic import BaseModel\n\n# ---- Functions ---\n\n\nclass Detection(BaseModel):\n    x_min: int\n    y_min: int\n    x_max: int\n    y_max: int\n    class_name: str\n    confidence: float\n\n\nclass Result(BaseModel):\n    detections: List[Detection] = []\n    time: float = 0.0\n    model: str\n\n\n@st.cache(show_spinner=True)\ndef make_dummy_request(model_url: str, model: str, image: Image) -&gt; Result:\n\"\"\"\n    This simulates a fake answer for you to test your application without having access to any other input from other teams\n    \"\"\"\n    # We do a dummy encode and decode pass to check that the file is correct\n    with io.BytesIO() as buffer:\n        image.save(buffer, format=\"PNG\")\n        buffer: str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        data = {\"model\": model, \"image\": buffer}\n\n    # We do a dummy decode\n    _image = data.get(\"image\")\n    _image = _image.encode(\"utf-8\")\n    _image = base64.b64decode(_image)\n    _image = Image.open(io.BytesIO(_image))  # type: Image\n    if _image.mode == \"RGBA\":\n        _image = _image.convert(\"RGB\")\n\n    _model = data.get(\"model\")\n\n    # We generate a random prediction\n    w, h = _image.size\n\n    detections = [\n        Detection(\n            x_min=random.randint(0, w // 2 - 1),\n            y_min=random.randint(0, h // 2 - 1),\n            x_max=random.randint(w // w, w - 1),\n            y_max=random.randint(h // 2, h - 1),\n            class_name=\"dummy\",\n            confidence=round(random.random(), 3),\n        )\n        for _ in range(random.randint(1, 10))\n    ]\n\n    # We return the result\n    result = Result(time=0.1, model=_model, detections=detections)\n\n    return result\n\n\n@st.cache(show_spinner=True)\ndef make_request(model_url: str, model: str, image: Image) -&gt; Result:\n\"\"\"\n    Process our data and send a proper request\n    \"\"\"\n    with io.BytesIO() as buffer:\n        image.save(buffer, format=\"PNG\")\n        buffer: str = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n        data = {\"model\": model, \"image\": buffer}\n\n        response = requests.post(\"{}/predict\".format(model_url), json=data)\n\n    if not response.status_code == 200:\n        raise ValueError(\"Error in processing payload, {}\".format(response.text))\n\n    response = response.json()\n\n    return Result.parse_obj(response)\n\n\ndef draw_preds(image: Image, detections: [Detection]):\n\n    class_names = list(set([detection.class_name for detection in detections]))\n\n    image_with_preds = image.copy()\n\n    # Define colors\n    colors = plt.cm.get_cmap(\"viridis\", len(class_names)).colors\n    colors = (colors[:, :3] * 255.0).astype(np.uint8)\n\n    # Define font\n    font = list(Path(\"/usr/share/fonts\").glob(\"**/*.ttf\"))[0].name\n    font = ImageFont.truetype(font=font, size=np.floor(3e-2 * image_with_preds.size[1] + 0.5).astype(\"int32\"))\n    thickness = (image_with_preds.size[0] + image_with_preds.size[1]) // 300\n\n    # Draw detections\n    for detection in detections:\n        left, top, right, bottom = detection.x_min, detection.y_min, detection.x_max, detection.y_max\n        score = float(detection.confidence)\n        predicted_class = detection.class_name\n        class_idx = class_names.index(predicted_class)\n\n        label = \"{} {:.2f}\".format(predicted_class, score)\n\n        draw = ImageDraw.Draw(image_with_preds)\n        label_size = draw.textsize(label, font)\n\n        top = max(0, np.floor(top + 0.5).astype(\"int32\"))\n        left = max(0, np.floor(left + 0.5).astype(\"int32\"))\n        bottom = min(image_with_preds.size[1], np.floor(bottom + 0.5).astype(\"int32\"))\n        right = min(image_with_preds.size[0], np.floor(right + 0.5).astype(\"int32\"))\n\n        if top - label_size[1] &gt;= 0:\n            text_origin = np.array([left, top - label_size[1]])\n        else:\n            text_origin = np.array([left, top + 1])\n\n        # My kingdom for a good redistributable image drawing library.\n        for r in range(thickness):\n            draw.rectangle([left + r, top + r, right - r, bottom - r], outline=tuple(colors[class_idx]))\n        draw.rectangle([tuple(text_origin), tuple(text_origin + label_size)], fill=tuple(colors[class_idx]))\n\n        if any(colors[class_idx] &gt; 128):\n            fill = (0, 0, 0)\n        else:\n            fill = (255, 255, 255)\n\n        draw.text(text_origin, label, fill=fill, font=font)\n\n        del draw\n\n    return image_with_preds\n\n\n# ---- Streamlit App ---\n\nst.title(\"Yolo v5 Companion App\")\n\nst.markdown(\n    \"A super nice companion application to send requests and parse results\\n\"\n    \"We wrap https://pytorch.org/hub/ultralytics_yolov5/\"\n)\n\n# ---- Sidebar ----\n\ntest_mode_on = st.sidebar.checkbox(label=\"Test Mode - Generate dummy answer\", value=False)\n\nst.sidebar.markdown(\"Enter the cluster URL\")\nmodel_url = st.sidebar.text_input(label=\"Cluster URL\", value=\"http://localhost:8000\")\n\n_model_url = model_url.strip(\"/\")\n\nif st.sidebar.button(\"Send 'is alive' to IP\"):\n    try:\n        health = requests.get(\"{}/health\".format(_model_url))\n        title = requests.get(\"{}/\".format(_model_url))\n        version = requests.get(\"{}/version\".format(_model_url))\n        describe = requests.get(\"{}/describe\".format(_model_url))\n\n        if health.status_code == 200:\n            st.sidebar.success(\"Webapp responding at {}\".format(_model_url))\n            st.sidebar.json({\"title\": title.text, \"version\": version.text, \"description\": describe.text})\n        else:\n            st.sidebar.error(\"Webapp not respond at {}, check url\".format(_model_url))\n    except ConnectionError:\n        st.sidebar.error(\"Webapp not respond at {}, check url\".format(_model_url))\n\n\n# ---- Main window ----\n\nst.markdown(\"## Inputs\")\nst.markdown(\"Select your model (Small, Medium or Large)\")\n\n# Data input\nmodel_name = st.radio(label=\"Model Name\", options=[\"yolov5s\", \"yolov5m\", \"yolov5l\"])\n\nst.markdown(\"Upload an image\")\n\nimage_file = st.file_uploader(label=\"Image File\", type=[\"png\", \"jpg\", \"tif\"])\n\nconfidence_threshold = st.slider(label=\"Confidence filter\", min_value=0.0, max_value=1.0, value=0.0, step=0.05)\n\n# UploadFile to PIL Image\nif image_file is not None:\n    image_file.seek(0)\n    image = image_file.read()\n    image = Image.open(io.BytesIO(image))\n\nst.markdown(\"Send the payload to {}/predict\".format(_model_url))\n\n# Send payload\nif st.button(label=\"SEND PAYLOAD\"):\n    if test_mode_on:\n        st.warning(\"Simulating a dummy request to {}\".format(model_url))\n        result = make_dummy_request(model_url=_model_url, model=model_name, image=image)\n    else:\n        result = make_request(model_url=_model_url, model=model_name, image=image)\n\n    st.balloons()\n\n    # Display results\n    st.markdown(\"## Display\")\n\n    st.text(\"Model : {}\".format(result.model))\n    st.text(\"Processing time : {}s\".format(result.time))\n\n    detections = [detection for detection in result.detections if detection.confidence &gt; confidence_threshold]\n\n    image_with_preds = draw_preds(image, detections)\n    st.image(image_with_preds, width=1024, caption=\"Image with detections\")\n\n    st.markdown(\"### Detection dump\")\n    for detection in result.detections:\n        st.json(detection.json())\n</code></pre> <p>Note</p> <p>Le test mode servait pour un ancien BE. Si vous avez tout fait dans l'ordre vous ne devriez pas en avoir besoin</p>"},{"location":"1_5_deployment_tp.html#construire-le-docker_1","title":"Construire le docker","text":"<pre><code>PROJECT_ID=$(gcloud config get-value project 2&gt; /dev/null)\ndocker build -t eu.gcr.io/${PROJECT_ID}/{your app name}:{your version} -f Dockerfile .\n</code></pre>"},{"location":"1_5_deployment_tp.html#tester-le-docker_1","title":"Tester le docker","text":"<p>Warning</p> <p>Malheureusement, sur github codespace cela ne semble pas fonctionner. Nous allons devoir partir du principe que cela fonctionne du premier coup ! Le mieux est donc de s'assurer que le app.py correspond \u00e0 la correction puis de passer \u00e0 la section suivante</p> <p>Au lieu de faire <code>streamlit run app.py</code>, vous pouvez lancer le docker localement et aller sur {ip}:8501 pour tester le docker</p> <pre><code>PROJECT_ID=$(gcloud config get-value project 2&gt; /dev/null)\ndocker run --rm -p 8501:8501 eu.gcr.io/${PROJECT_ID}/{your app name}:{your version}\n</code></pre> <p>Vous pouvez vous rendre sur l'ip de la machine sur le port 8501</p> <p>Indiquez l'ip de la machine port 8000 \u00e0 gauche</p>"},{"location":"1_5_deployment_tp.html#pousser-le-docker-sur-google-container-registry","title":"Pousser le docker sur google container registry","text":"<pre><code>gcloud auth configure-docker\ndocker push eu.gcr.io/${PROJECT_ID}/{your-name}-frontend:{your version}\n</code></pre>"},{"location":"1_5_deployment_tp.html#liens-utiles_1","title":"Liens Utiles","text":"<ul> <li>Doc Streamlit</li> </ul>"},{"location":"1_5_deployment_tp.html#4-deployer-le-modele-et-lux-sur-linstance-gcp","title":"4 - D\u00e9ployer le mod\u00e8le et l'UX sur l'instance GCP","text":"<p>Nous allons cr\u00e9er une machine virtuelle dans laquelle nous allons lancer les deux containers</p>"},{"location":"1_5_deployment_tp.html#41-creation-de-la-vm","title":"4.1 Cr\u00e9ation de la VM","text":"<p>Nous allons directement cr\u00e9er une machine avec le container du mod\u00e8le d\u00e9j\u00e0 lanc\u00e9</p> <p>Commen\u00e7ons par cr\u00e9er une instance GCP bien configur\u00e9e depuis laquelle se connecter:</p> <p>N'oubliez pas de renommer le nom de votre instance</p> <pre><code>export INSTANCE_NAME=\"tp-deployment-{yourgroup}-{yourname}\" # Don't forget to replace values !\n</code></pre> <pre><code>gcloud compute instances create $INSTANCE_NAME \\\n--zone=\"europe-west1-b\" \\\n--machine-type=\"n1-standard-2\" \\\n--image-family=\"common-cpu\" \\\n--image-project=\"deeplearning-platform-release\" \\\n--maintenance-policy=TERMINATE \\\n--scopes=\"storage-rw\" \\\n--boot-disk-size=75GB\n</code></pre> <p>R\u00e9cuperez l'ip publique de la machine (via l'interface google cloud ou bien en faisant <code>gcloud compute instances list | grep {votre instance}</code> et notez l\u00e0 bien</p> <p>Depuis le github codespace, connectez vous \u00e0 la machine</p> <pre><code>    gcloud compute ssh {user}@{instance}\n</code></pre>"},{"location":"1_5_deployment_tp.html#42-execution-des-containers","title":"4.2 Execution des containers","text":"<p>Hint</p> <p>A executer dans la VM GCP</p> <p>On va utiliser <code>docker compose</code> pour lancer les deux applications en simultan\u00e9 de sorte \u00e0 ce qu'elles communiquent</p> <p>Plus d'infos sur docker compose</p> <ul> <li>Fermez tous les dockers etc.</li> <li>Cr\u00e9ez un fichier <code>docker-compose.yml</code></li> </ul> <p>Sur votre codespace, cr\u00e9ez ce fichier et modifiez le nom des images avec celles que vous avez utilis\u00e9es (respectivement model et frontend)</p> <pre><code>version: '3'\nservices:\nyolo:\nimage: \"eu.gcr.io/third-ridge-138414/yolo-v5:1.2\"\nports:\n- \"8000:8000\"\nhostname: yolo\nstreamlit:\nimage: \"eu.gcr.io/third-ridge-138414/yolo-v5-streamlit:1.2\"\nports:\n- \"8501:8501\"\nhostname: streamlit\n</code></pre> <p>Copiez ensuite ce texte sur la VM dans un fichier <code>docker-compose.yml</code> (exemple : via nano)</p> <p>On constate qu'on d\u00e9clare 2 services: - 1 service \"yolo\" - 1 service \"streamlit\"</p> <p>On d\u00e9clare aussi les ports ouverts de chaque application</p> <p>Maintenant... comment lancer les deux applications ?</p> <p><code>docker-compose up</code> dans le dossier o\u00f9 se trouve votre <code>docker-compose.yml</code></p> <p>Hint</p> <p>Si <code>docker-compose</code> ne fonctionne pas, <code>sudo apt -y install docker-compose</code></p> <p>Normalement: - le service de mod\u00e8le est accessible sur le port 8000 de la machine - le service streamlit est accessible sur le port 8501 de la machine - vous devez indiquer l'hostname \"yolo\" pour communiquer entre streamlit et le mod\u00e8le. En effet, les services sont accessibles via un r\u00e9seau sp\u00e9cial \"local\" entre tous les containers lanc\u00e9s via docker-compose</p>"},{"location":"1_5_deployment_tp.html#acces-a-la-vm","title":"Acc\u00e8s \u00e0 la VM","text":"<p>Hint</p> <p>Cela ne risque de fonctionner que en 4G</p> <p>Connectez vous via l'IP publique de la machine via votre navigateur web, sur le port 8501 : <code>http://ip-de-la-machine:8501</code></p> <p>Vous devriez pouvoir acc\u00e9der \u00e0 votre d\u00e9ploiement !</p>"},{"location":"1_5_deployment_tp.html#conclusion","title":"Conclusion","text":"<p>\ud83c\udf89 Bravo ! \ud83c\udf89</p> <p>Vous avez d\u00e9ploy\u00e9 votre premier mod\u00e8le en production !</p>"},{"location":"1_5_distributed.html","title":"Evolution of Data Management Systems","text":""},{"location":"1_5_distributed.html#fundamental-concepts-methods-and-applications","title":"Fundamental Concepts, Methods and Applications","text":"<p>In this three part class, students will cover the history of data management systems, from file systems to databases to distributed cloud storage. This class is given over the length of the Data Engineering course. Questions from the first two parts are integrated into the exam on cloud computing, and questions from the Cloud DMS section are integrated into the Dask notebook evaluation.</p>"},{"location":"1_5_distributed.html#objectives","title":"Objectives","text":"<p>The objectives of this course are: - Introduce the fundamental concepts - Describe, in a synthetic way, the main characteristics of the evolution of DMS (Data Management Systems) - Highlight targeted application classes.</p>"},{"location":"1_5_distributed.html#key-words","title":"Key Words","text":"<p>Data Management Systems, Uni-processor DBMS, Parallel DBMS, Data Integration Systems,Big Data, Cloud  Data Management Systems, High Performance, Scalability, Elasticity, Multi-store/Poly-store Systems</p>"},{"location":"1_5_distributed.html#targeted-skills","title":"Targeted Skills","text":"<ul> <li>Effectively exploit the DMS according to the environment (uniprocessor, parallel, distributed, cloud) in a perspective of decision support within an organization.</li> <li>Ability to choose, in a relevant way, a DMS in multiple environments for an optimal functioning of the applications of an organization</li> </ul>"},{"location":"1_5_distributed.html#indicative-program","title":"Indicative Program","text":"<ol> <li> <p>Introduction to Main Problems of Data Management</p> <ul> <li>From File Management Systems FMS to Database MS DBMS</li> <li>Motivations, Objectives, Organizations &amp; Drawbacks</li> <li>Databases &amp; Rel. DBMS: Motivations &amp; Objectives</li> <li>Resources:<ul> <li>Introduction</li> <li>SGF - File Systems</li> <li>Views - Relational Systems</li> <li>File Organization</li> </ul> </li> </ul> </li> <li> <p>Parallel Database Systems</p> <ul> <li>Objectives and Parallel Architecture Models</li> <li>Data Partitioning Strategies</li> <li>Parallel Query Processing</li> <li>Motivations &amp; Objectives</li> <li>Characteristics and Challenges</li> <li>Resources:<ul> <li>Parallel DBMS</li> <li>Parallel Queries</li> </ul> </li> </ul> </li> <li> <p>From Distributed DB to Data Integration Systems DIS</p> <ul> <li>An Ex. of DBD, Motivations &amp; Objectives</li> <li>Designing of DDB</li> <li>Distributed Query Processing</li> <li>An Ex. of DIS</li> <li>Motivations &amp; Objectives</li> <li>Mediator-Adapters Architecture</li> <li>Design of a Global Schema (GAV, LAV)</li> <li>Query Processing Methodologies</li> </ul> </li> <li> <p>Cloud Data Management Systems CDMS</p> <ul> <li>Motivations and Objectives</li> <li>Main Characteristics of Big Data and CDMS</li> <li>Classification of Cloud Data Management Systems CDMS</li> <li>Advantages and Weakness of Parallel RDBMS and CDMS</li> <li>Comparison between Parallel RDBMS and CDMS</li> <li>Introduction to Multi-store/Ploystore Systems</li> <li>Resources:<ul> <li>Distributed DBMS - Chapter 1</li> <li>Distributed DBMS - Chapter 2</li> <li>Distributed DBMS - Chapter 3</li> <li>Systems for integrating heterogeneous and distributed data</li> <li>Integration Systems complement</li> <li>MapReduce examples</li> </ul> </li> </ul> </li> <li> <p>Conclusion</p> <ul> <li>Maturity of Cloud DMS</li> <li>Key Criteria for Choosing a Data Management System</li> </ul> </li> </ol>"},{"location":"1_5_distributed.html#additional-reading","title":"Additional Reading","text":"<ol> <li> <p>Principles of Distributed Database Systems,  M. Tamer Ozsu  and Patrick Valduriez; Springer-Verlag ;  Fourth Edition,  December 2019.</p> </li> <li> <p>Data Management in the Cloud: Challenges and Opportunities Divyakant Agrawal, Sudipto Das, and Amr El Abbadi; Synthesis Lectures on Data Management, December 2012, Vol. 4, No. 6 , Pages 1-138.</p> </li> <li> <p>Query Processing in Parallel Relational Database Systems; H. Lu, B.-C Ooi and K.-L. Tan; IEEE Computer Society Press, CA, USA, 1994.</p> </li> <li> <p>Traitement parall\u00e8le dans les bases de donn\u00e9es relationnelles : concepts, m\u00e9thodes et applications Abdelkader Hameurlain, Pierre Bazex, Franck Morvan; C\u00e9padu\u00e8s Editions,  Octobre 1996.  </p> </li> </ol>"},{"location":"1_5_kub_intro.html","title":"Kubernetes: Zero to Jupyterhub using Google Kubernetes Engine","text":""},{"location":"1_5_kub_intro.html#what-is-jupyterhub","title":"What is JupyterHub","text":"<p>JupyterHub brings the power of notebooks to groups of users. It gives users access to computational environments and resources without burdening the users with installation and maintenance tasks. Users - including students, researchers, and data scientists - can get their work done in their own workspaces on shared resources which can be managed efficiently by system administrators.</p> <p>JupyterHub runs in the cloud or on your own hardware, and makes it possible to serve a pre-configured data science environment to any user in the world. It is customizable and scalable, and is suitable for small and large teams, academic courses, and large-scale infrastructure. Key features of JupyterHub</p> <p>Customizable - JupyterHub can be used to serve a variety of environments. It supports dozens of kernels with the Jupyter server, and can be used to serve a variety of user interfaces including the Jupyter Notebook, Jupyter Lab, RStudio, nteract, and more.</p> <ul> <li> <p>Flexible - JupyterHub can be configured with authentication in order to provide access to a subset of users. Authentication is pluggable, supporting a number of authentication protocols (such as OAuth and GitHub).</p> </li> <li> <p>Scalable - JupyterHub is container-friendly, and can be deployed with modern-day container technology. It also runs on Kubernetes, and can run with up to tens of thousands of users.</p> </li> <li> <p>Portable - JupyterHub is entirely open-source and designed to be run on a variety of infrastructure. This includes commercial cloud providers, virtual machines, or even your own laptop hardware.</p> </li> </ul> <p>The foundational JupyterHub code and technology can be found in the JupyterHub repository. This repository and the JupyterHub documentation contain more information about the internals of JupyterHub, its customization, and its configuration.</p>"},{"location":"1_5_kub_intro.html#zero-to-jupyterhub-using-kubernetes","title":"Zero to Jupyterhub using Kubernetes","text":"<p>JupyterHub allows users to interact with a computing environment through a webpage. As most devices have access to a web browser, JupyterHub makes it is easy to provide and standardize the computing environment of a group of people (e.g., for a class of students or an analytics team).</p> <p>This project will help you set up your own JupyterHub on a cloud and leverage the clouds scalable nature to support large groups of users. Thanks to Kubernetes, we are not tied to a specific cloud provider.</p>"},{"location":"1_5_kub_intro.html#instructions","title":"Instructions","text":"<ul> <li> <p>Go here and follow the instructions</p> </li> <li> <p>Use Google Kubernetes Engine to setup your cluster</p> </li> </ul> <p>Info</p> <p>You will use the same method later in the year to setup a Dask Kubernetes cluster using helm</p> <ul> <li>Give some people the public IP of your cluster so that they can connect to it... try to make it scale !</li> </ul>"},{"location":"1_6_conclusion.html","title":"Recap' &amp; Conclusion","text":"<p>Link to slides</p> <p> </p>"},{"location":"1_7_readings.html","title":"Readings","text":""},{"location":"1_7_readings.html#about-cloud-computing","title":"About Cloud Computing","text":"<ul> <li> <p>Buyya, R., Srirama, S. N., Casale, G., Calheiros, R., Simmhan, Y., Varghese, B., ... &amp; Toosi, A. N. (2018). A manifesto for future generation cloud computing: Research directions for the next decade. ACM computing surveys (CSUR), 51(5), 1-38.</p> </li> <li> <p>On sustainable data centers and energy use (intro)</p> </li> <li> <p>The NIST Definitions of Cloud Computing</p> </li> <li> <p>Open Data: Open Sentinel 2 archive on AWS</p> </li> <li> <p>Environmental Impact of Cloud vs On Premise</p> </li> <li> <p>Environmental Impact of cloud vs on-premise medium blog post</p> </li> <li> <p>Paper from Natural Resources Defense Council on Cloud vs On-Premise</p> </li> <li> <p>Anecdotes about Cloud Computing</p> </li> </ul>"},{"location":"1_7_readings.html#about-containers","title":"About Containers","text":"<ul> <li> <p>Docker whitepaper: Docker and the way of the Devops</p> </li> <li> <p>What exactly is Docker ? Simple explanation from a medium blog post</p> </li> </ul>"},{"location":"1_7_readings.html#about-orchestration","title":"About Orchestration","text":"<ul> <li> <p>Verma, A., Pedrosa, L., Korupolu, M., Oppenheimer, D., Tune, E., &amp; Wilkes, J. (2015, April). Large-scale cluster management at Google with Borg. In Proceedings of the Tenth European Conference on Computer Systems (pp. 1-17).</p> </li> <li> <p>Kubernetes Comic to learn about Kubernetes in a fun way https://cloud.google.com/kubernetes-engine/kubernetes-comic</p> </li> </ul>"},{"location":"2_1_overview.html","title":"Introduction to Data Distribution","text":"<p>Course Introduction</p>"},{"location":"2_1_overview.html#course-overview","title":"Course Overview","text":"<ul> <li>Data Distribution &amp; Big Data Processing</li> </ul> <p>Harnessing the complexity of large amounts of data is a challenge in itself. </p> <p>But Big Data processing is more than that: originally characterized by the 3 Vs of Volume, Velocity and Variety,  the concepts popularized by Hadoop and Google requires dedicated computing solutions (both software and infrastructure),  which will be explored in this module.</p>"},{"location":"2_1_overview.html#objectives","title":"Objectives","text":"<p>By the end of this module, participants will be able to:</p> <ul> <li>Understand the differences and usage between main distributed computing architectures (HPC, Big Data, Cloud, CPU vs GPGPU)</li> <li>Implement the distribution of simple operations via the Map/Reduce principle in PySpark</li> <li>Understand the principle of Kubernetes</li> <li>Deploy a Big Data Processing Platform on the Cloud</li> <li>Implement the distribution of data wrangling/cleaning and training machine learning algorithms using PyData stack, Jupyter notebooks and Dask</li> </ul>"},{"location":"2_1_overview.html#program","title":"Program","text":""},{"location":"2_1_overview.html#big-data-distributed-computing-3h","title":"Big Data &amp; Distributed Computing (3h)","text":"<ul> <li>Introduction to Big Data and its ecosystem (1h)</li> <li>What is Big Data?</li> <li>Legacy \u201cBig Data\u201d ecosystem</li> <li>Big Data use cases</li> <li>Big Data to Machine Learning</li> <li>Big Data platforms, Hadoop &amp; Beyond (2h)</li> <li>Hadoop, HDFS and MapReduce,</li> <li>Datalakes, Data Pipelines</li> <li>From HPC to Big Data to Cloud and High Performance Data Analytics </li> <li>BI vs Big Data</li> <li>Hadoop legacy: Spark, Dask, Object Storage ...</li> </ul>"},{"location":"2_1_overview.html#spark-35h","title":"Spark (3.5h)","text":"<ul> <li>Spark Introduction (1h)</li> <li>Play with MapReduce through Spark (Notebook on small datasets) (2.5h)</li> </ul>"},{"location":"2_1_overview.html#kubernetes-dask-35h","title":"Kubernetes &amp; Dask (3.5h)","text":"<ul> <li>Containers Orchestration (1h)</li> <li>Kubernetes &amp; CaaS &amp; PaaS (Databricks, Coiled)</li> <li>Play with Kubernetes (if we have time)</li> <li>Dask Presentation (1h)</li> <li>Deploy a Data processing platform on the Cloud based on Kubernetes and Dask (1.5h)</li> <li>Exercise: DaskHub or Dask Kubernetes or Pangeo</li> </ul>"},{"location":"2_1_overview.html#evaluation-7h","title":"Evaluation (7h)","text":"<ul> <li>Prerequisite: Pangeo platform deployed before</li> <li>Clean big amounts of data using Dask in the cloud (3h)</li> <li>Train machine learning models in parallel (hyper parameter search) (3h)</li> <li>Notebook with cell codes to fill or answers to give</li> </ul> <p>Evaluation introduction slides</p>"},{"location":"2_2_functional.html","title":"Functional Programming","text":"<p>This section of the course is not given this year.</p>"},{"location":"2_2_functional.html#functional-programming-for-distributed-data","title":"Functional Programming for Distributed Data","text":"<p>Link to slides</p> <p> </p>"},{"location":"2_2_functional.html#introduction-to-julia","title":"Introduction to Julia","text":"<p>As the first exercise, you'll need to install Julia and IJulia locally or make a working Julia Colab Notebook. While Colab is sufficient for today's exercises, it is recommended to make a local installation:</p> <p>Julia download Julia kernel for Jupyter</p> <p>Here is a Colab template from this Github repository which will install the Julia kernel for a single Colab instance.</p> <p>Once you have a Julia Jupyter kernel, follow this Julia for Pythonistas notebook.</p> <p>Github Colab</p>"},{"location":"2_2_functional.html#functional-programming-in-julia","title":"Functional Programming in Julia","text":"<p>Julia documentation explaining:</p> <ul> <li>Functions, showing that they are first-class</li> <li>the <code>map</code> function which is a higher-order function</li> <li>distributed computing allowing for transfer of functions between threads or workers</li> </ul>"},{"location":"2_2_functional.html#distributed-data-in-julia","title":"Distributed Data in Julia","text":"<p>Julia's base language supports distributed calculation but there are a few packages which facilitate data processing tasks over distributed data: </p> <ul> <li>DistributedArrays - A general Array type which can be distributed over multiple workers.</li> <li>JuliaDB - A data structuring package which automatically handles distributed data storage and computation</li> <li>Spark.jl - A Julia interface to Apache Spark. Related blog post.</li> </ul>"},{"location":"2_2_functional.html#map-reduce-exercise","title":"Map Reduce Exercise","text":"<p>The second part of this class is an interactive notebook in the Julia language covering the MapReduce programming framework, from simple addition queries to a grep example.</p> <p>MapReduce notebook</p> <p>MapReduce notebook on Colab (requires adding Julia kernel installation)</p>"},{"location":"2_3_mapreduce.html","title":"Hadoop and MapReduce","text":"<p>In this class, we start with an overview of the Big Data ecosystem, contextualizing Hadoop, No-SQL Databases, and Business Intelligence tools. We then cover Hadoop and the HDFS in detail with a simple MapReduce example.</p> <ul> <li>Introduction to Big Data and its ecosystem (1h)</li> <li>What is Big Data?</li> <li>Legacy \u201cBig Data\u201d ecosystem</li> <li>Big Data use cases</li> <li>Big Data to Machine Learning</li> <li>Big Data platforms, Hadoop &amp; Beyond (2h)</li> <li>Hadoop, HDFS and MapReduce,</li> <li>Datalakes, Data Pipelines</li> <li>From HPC to Big Data to Cloud and High Performance Data Analytics </li> <li>BI vs Big Data</li> <li>Hadoop legacy: Spark, Dask, Object Storage ...</li> </ul> <p>It contains also a short interactive exercise using Python Map Reduce.</p>"},{"location":"2_4_spark.html","title":"Spark","text":"<p>In this class, we cover the Apache Spark framework, explaining Resilient Distributed Datasets, SparkSQL, Spark MLLib, and how to interact with a Spark cluster. We use PySpark in a Jupyter notebook to explore RDDs and see an example of distributed K-Means.</p> <p>Spark introduction</p> <p>Spark notebook</p> <p>Spark notebook on Colab</p>"},{"location":"2_5_cloud.html","title":"Evolution of Data Management Systems","text":""},{"location":"2_5_cloud.html#fundamental-concepts-methods-and-applications","title":"Fundamental Concepts, Methods and Applications","text":"<p>In this three part class, students will cover the history of data management systems, from file systems to databases to distributed cloud storage. This class is given over the length of the Data Engineering course. Questions from the first two parts are integrated into the exam on cloud computing, and questions from the Cloud DMS section are integrated into the Dask notebook evaluation.</p>"},{"location":"2_5_cloud.html#objectives","title":"Objectives","text":"<p>The objectives of this course are: - Introduce the fundamental concepts - Describe, in a synthetic way, the main characteristics of the evolution of DMS (Data Management Systems) - Highlight targeted application classes.</p>"},{"location":"2_5_cloud.html#key-words","title":"Key Words","text":"<p>Data Management Systems, Uni-processor DBMS, Parallel DBMS, Data Integration Systems,Big Data, Cloud  Data Management Systems, High Performance, Scalability, Elasticity, Multi-store/Poly-store Systems</p>"},{"location":"2_5_cloud.html#targeted-skills","title":"Targeted Skills","text":"<ul> <li>Effectively exploit the DMS according to the environment (uniprocessor, parallel, distributed, cloud) in a perspective of decision support within an organization.</li> <li>Ability to choose, in a relevant way, a DMS in multiple environments for an optimal functioning of the applications of an organization</li> </ul>"},{"location":"2_5_cloud.html#indicative-program","title":"Indicative Program","text":"<ol> <li> <p>Introduction to Main Problems of Data Management</p> <ul> <li>From File Management Systems FMS to Database MS DBMS</li> <li>Motivations, Objectives, Organizations &amp; Drawbacks</li> <li>Databases &amp; Rel. DBMS: Motivations &amp; Objectives</li> <li>Resources:<ul> <li>Introduction</li> <li>SGF - File Systems</li> <li>Views - Relational Systems</li> <li>File Organization</li> </ul> </li> </ul> </li> <li> <p>Parallel Database Systems</p> <ul> <li>Objectives and Parallel Architecture Models</li> <li>Data Partitioning Strategies</li> <li>Parallel Query Processing</li> <li>Motivations &amp; Objectives</li> <li>Characteristics and Challenges</li> <li>Resources:<ul> <li>Parallel DBMS</li> <li>Parallel Queries</li> </ul> </li> </ul> </li> <li> <p>From Distributed DB to Data Integration Systems DIS</p> <ul> <li>An Ex. of DBD, Motivations &amp; Objectives</li> <li>Designing of DDB</li> <li>Distributed Query Processing</li> <li>An Ex. of DIS</li> <li>Motivations &amp; Objectives</li> <li>Mediator-Adapters Architecture</li> <li>Design of a Global Schema (GAV, LAV)</li> <li>Query Processing Methodologies</li> </ul> </li> <li> <p>Cloud Data Management Systems CDMS</p> <ul> <li>Motivations and Objectives</li> <li>Main Characteristics of Big Data and CDMS</li> <li>Classification of Cloud Data Management Systems CDMS</li> <li>Advantages and Weakness of Parallel RDBMS and CDMS</li> <li>Comparison between Parallel RDBMS and CDMS</li> <li>Introduction to Multi-store/Ploystore Systems</li> <li>Resources:<ul> <li>Distributed DBMS - Chapter 1</li> <li>Distributed DBMS - Chapter 2</li> <li>Distributed DBMS - Chapter 3</li> <li>Systems for integrating heterogeneous and distributed data</li> <li>Integration Systems complement</li> <li>MapReduce examples</li> </ul> </li> </ul> </li> <li> <p>Conclusion</p> <ul> <li>Maturity of Cloud DMS</li> <li>Key Criteria for Choosing a Data Management System</li> </ul> </li> </ol>"},{"location":"2_5_cloud.html#additional-reading","title":"Additional Reading","text":"<ol> <li> <p>Principles of Distributed Database Systems,  M. Tamer Ozsu  and Patrick Valduriez; Springer-Verlag ;  Fourth Edition,  December 2019.</p> </li> <li> <p>Data Management in the Cloud: Challenges and Opportunities Divyakant Agrawal, Sudipto Das, and Amr El Abbadi; Synthesis Lectures on Data Management, December 2012, Vol. 4, No. 6 , Pages 1-138.</p> </li> <li> <p>Query Processing in Parallel Relational Database Systems; H. Lu, B.-C Ooi and K.-L. Tan; IEEE Computer Society Press, CA, USA, 1994.</p> </li> <li> <p>Traitement parall\u00e8le dans les bases de donn\u00e9es relationnelles : concepts, m\u00e9thodes et applications Abdelkader Hameurlain, Pierre Bazex, Franck Morvan; C\u00e9padu\u00e8s Editions,  Octobre 1996.  </p> </li> </ol>"},{"location":"2_5_dask.html","title":"Dask on Kubernetes","text":"<p>In this class, we focus on getting a Dask cluster running in Kubernetes, which we will then use in the Dask project. Dask is a parallel computing library in Python which integrates well with machine learning tools like scikit-learn.</p> <p>This class builds on the orchestration class, going into further detail on K8S specifics.</p> <p>Kubernetes</p> <p>Dask presentation</p> <p>Students will use GCP for this class. Be sure to stop your cluster after class to conserve GCP credits.</p> <p>Additional resources can be found in the dask documentation.</p>"},{"location":"2_5_dask.html#deploying-a-dask-hub","title":"Deploying a Dask Hub","text":"<p>This material is taken from the following docs:</p> <ul> <li>https://docs.dask.org/en/latest/setup/kubernetes-helm.html</li> <li>https://zero-to-jupyterhub.readthedocs.io/en/latest/kubernetes/setup-kubernetes.html</li> <li>https://zero-to-jupyterhub.readthedocs.io/en/latest/kubernetes/setup-helm.html</li> </ul>"},{"location":"2_5_dask.html#creating-a-kubernetes-cluster","title":"Creating a Kubernetes Cluster","text":"<p>First, you need to enable the Kubernetes API if not already done:</p> <ul> <li>Go to console.cloud.google.com</li> <li>Select the Kubernetes Engine in the menu</li> <li>Enable the API if not already done.</li> </ul> <p>Then you'll need a terminal with <code>gcloud</code> and <code>kubectl</code>. The simplest is just to use the Google Cloud Shell from console.cloud.google.com. If you prefer, you can follow the links above to find how to install everything on your computer.</p> <p>Ask Google Cloud to create a managed Kubernetes cluster and a default node pool to get nodes from:</p> <pre><code>gcloud container clusters create \\\n  --machine-type n1-standard-4 \\\n  --enable-autoscaling \\\n  --min-nodes 1 \\\n  --max-nodes 10 \\\n  --num-nodes 1 \\\n  --zone europe-west1-b \\\n  --cluster-version 1.23 \\\n  dask-hub-k8s\n</code></pre> <p>Yhis will take a few minutes (maybe 2 or 3).</p> <pre><code>gcloud container clusters list\n</code></pre> <p>You can then test if the cluster is running:</p> <pre><code>kubectl get node\n</code></pre> <p>Then get permissions to perform all administrative actions needed.</p> <p>\u26a0\ufe0fDon't forget to replace your email below.\u26a0\ufe0f</p> <pre><code>kubectl create clusterrolebinding cluster-admin-binding \\\n  --clusterrole=cluster-admin \\\n  --user=&lt;GOOGLE-EMAIL-ACCOUNT&gt;\n</code></pre>"},{"location":"2_5_dask.html#setting-up-helm","title":"Setting up Helm","text":"<p>From your Google Cloud Shell or terminal:</p> <pre><code>curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash\nhelm list\n</code></pre> <p>should return:</p> <pre><code>NAME    NAMESPACE       REVISION        UPDATED STATUS  CHART   APP VERSION\n</code></pre>"},{"location":"2_5_dask.html#helm-install-a-dask-hub","title":"Helm install a Dask Hub","text":"<p>Default Daskhub configuration uses dask-gateway, which is here to handle multiple users with fine grain authorisations. We don't need this, and it iss a little more complicated setup than what we'll do.</p> <p>Instead, we'll deploy a Daskhub with dask-kubernetes, which assumes some authorisations inside the Pods of the Kubernetes cluster (potential security leak), but is more straightforward for our usage.</p> <p>Verify that you\u2019ve set up a Kubernetes cluster and added Dask\u2019s helm charts:</p> <pre><code>helm repo add dask https://helm.dask.org/\nhelm repo update\n</code></pre> <p>Generate token to configure Jupyterhub:</p> <pre><code>openssl rand -hex 32  &gt; token1.txt\ncat token1.txt\n</code></pre> <p>Create the file below (for example using vim or cloud shell editor) and substitute the  value. <pre><code># file: daskhub-config.yaml\njupyterhub:\n  proxy:\n    secretToken: \"&lt;token-1&gt;\"\n  scheduling:\n    podPriority:\n      enabled: true\n    userPlaceholder:\n      # Specify three dummy user pods will be used as placeholders\n      replicas: 1\n    userScheduler:\n      enabled: true\n  singleuser:\n    serviceAccountName: daskkubernetes\n    image:\n      name: guillaumeeb/pangeo-ml-notebook # Image to use for singleuser environment. Must include dask-kubernetes.\n      tag: 2021.11.14\n\ndask-gateway:\n  enabled: false\n  gateway:\n    auth:\n      type: simple\n      simple:\n        password: \"unused\"\n\ndask-kubernetes:\n  enabled: true\n</code></pre> <p>Now we just install Dask Hub:</p> <pre><code>helm upgrade --wait --install --render-subchart-notes \\\n    --namespace daskhub \\\n    --create-namespace \\\n    dhub dask/daskhub \\\n    --values=daskhub-config.yaml\n</code></pre> <p>This will again take a few minutes.</p> <pre><code>helm list -n daskhub\n</code></pre> <p>Check install and go to Jupyter!</p> <p>To get the public IP of your hub deployment:</p> <pre><code>kubectl --namespace=daskhub get service proxy-public\n</code></pre> <p>Get the external IP, and open it in your browser. You should be able to login with any username/password Ensure Dask is working, and K8S mecanisms too!</p>"},{"location":"2_5_dask.html#create-a-dask-kubernetes-cluster","title":"Create a dask-kubernetes cluster","text":"<p>Create a yaml file within the Jupyterhub interface:</p> <pre><code># worker-spec.yaml\n\nkind: Pod\nmetadata:\n  labels:\n    foo: bar\nspec:\n  restartPolicy: Never\n  containers:\n  - image: guillaumeeb/pangeo-ml-notebook:2021.11.14\n    imagePullPolicy: IfNotPresent\n    args: [dask-worker, --nthreads, '2', --no-dashboard, --memory-limit, 6GB, --death-timeout, '60']\n    name: dask\n    env:\n      - name: EXTRA_PIP_PACKAGES\n        value: xgboost\n    resources:\n      limits:\n        cpu: \"2\"\n        memory: 6G\n      requests:\n        cpu: \"1.7\"\n        memory: 6G\n</code></pre> <p>Just open a notebook in your newly created Dask enabled hub, and try to copy and past the following cells:</p> <p>Set some config to ease usage.</p> <pre><code>import dask\nimport dask.distributed  # populate config with distributed defaults\nimport dask_kubernetes\n\ndask.config.set({\"kubernetes.worker-template-path\": \"worker-spec.yaml\"})\ndask.config.set({\"distributed.dashboard.link\": \"{JUPYTERHUB_SERVICE_PREFIX}proxy/{port}/status\"})\n</code></pre> <p>Create a cluster object.</p> <pre><code>from dask_kubernetes import KubeCluster\n\ncluster = KubeCluster(deploy_mode='local') # Scheduler is started in the notebook process\ncluster\n</code></pre> <p>This should display a fancy widget. You can open the Dask Dashboard from here.</p> <p>Now scale the cluster to get Dask-workers and connect to it.</p> <pre><code>cluster.scale(20)\n</code></pre> <pre><code>from distributed import Client\n\nclient = Client(cluster)\nclient\n</code></pre> <p>What's happening in your K8S cluster after some seconds/minutes? Launch some computation, what about Pi?</p> <p>We'll use Dask array, a Numpy extension, for this:</p> <pre><code>import dask.array as da\n\nsample = 10_000_000_000  # &lt;- this is huge!\nxxyy = da.random.uniform(-1, 1, size=(2, sample))\nnorm = da.linalg.norm(xxyy, axis=0)\nsumm = da.sum(norm &lt;= 1)\ninsiders = summ.compute()\npi = 4 * insiders / sample\nprint(\"pi ~= {}\".format(pi))\n</code></pre> <p>How many workers did you get? Why?</p> <p>Now just close the cluster.</p> <pre><code>cluster.close()\n</code></pre> <p>What happens after a few minutes?</p>"},{"location":"2_5_dask.html#deleting-a-kubernetes-cluster","title":"Deleting a Kubernetes Cluster","text":"<p>Get your cluster name and region</p> <pre><code>gcloud container clusters list\n</code></pre> <p>Delete your kubernetes cluster</p> <pre><code>gcloud container clusters delete &lt;YOUR_CLUSTER_NAME&gt; --region &lt;YOUR_CLUSTER_REGION&gt;\n</code></pre>"},{"location":"2_6_project.html","title":"Project - Dask","text":"<p>The evaluation for this class is a Dask notebook. You should run this notebook on a Daskhub using Kubernetes, like in the Dask on Kubernetes class. You should complete the exercises and answer the questions in the notebook, then turn it in through the LMS. You may work with a partner, in this case make sure to specify in your notebook that you worked together.</p> <p>The notebook is due on March 17, 2023 by midnight.</p> <p>Dask tutorial, if needed</p> <p>Evaluation notebook</p>"}]}